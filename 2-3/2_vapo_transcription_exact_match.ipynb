{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g65MmV_HKi7I"
      },
      "source": [
        "## VAPO Transcription Example 2-3\n",
        "\n",
        "- [Original Reference](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk_custom_metric.ipynb) from [Ivan Nardini](https://github.com/inardini)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HKyj5KwYePX"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This notebook demonstrates how to leverage Vertex AI prompt optimizer to optimize a simple prompt for a Gemini model using your own metric. The goal is to use Vertex AI prompt optimizer to find a new prompt template that generates better responses based on your own optimization metric.\n",
        "\n",
        "This tutorial uses the following Google Cloud services and resources:\n",
        "\n",
        "- Generative AI on Vertex AI\n",
        "- Vertex AI prompt optimizer\n",
        "- Vertex AI Gen AI evaluation\n",
        "- Vertex AI Custom job\n",
        "- Cloud Run\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "1. Define the prompt template you want to optimize.\n",
        "2. Prepare the prompt optimization dataset.\n",
        "3. Define and deploy your own custom evaluation metric on Cloud function.\n",
        "4. Set optimization mode and steps.\n",
        "5. Run the automatic prompt optimization job.\n",
        "6. Collect the best prompt template and eval metric.\n",
        "7. Validate the best prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## 2. Before you start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enFkr5SXo36F",
        "outputId": "8ae56683-0c3a-4c65-ade6-9e869d3eabf8"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet 'google-cloud-aiplatform[evaluation]'\n",
        "%pip install --upgrade --quiet 'plotly' 'asyncio' 'tqdm' 'tenacity' 'etils' 'importlib_resources' 'fsspec' 'gcsfs' 'nbformat>=4.2.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "RrKXzTVMDKb4",
        "outputId": "372c1d2c-0637-41b7-d8e6-927f30e6c31a"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py\n",
        "import vapo_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55e2195ce2d",
        "outputId": "d5eea223-8c01-4620-9940-59dca6475f4e"
      },
      "outputs": [],
      "source": [
        "! mkdir -p ./tutorial/utils && wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py -P ./tutorial/utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyKGtVQjgx13",
        "outputId": "6d46cf9a-c242-4e4c-f0ff-6dc41866ed80"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    try:\n",
        "        import google.auth\n",
        "        import google.auth.transport.requests\n",
        "        from google.colab import auth\n",
        "\n",
        "        auth.authenticate_user()\n",
        "        creds, project = google.auth.default()\n",
        "        authentication = google.auth.transport.requests.Request()\n",
        "        if creds.token:\n",
        "            print(\"Authentication successful.\")\n",
        "        else:\n",
        "            print(\"Authentication successful, but no token was returned.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Colab authentication: {e}\")\n",
        "\n",
        "! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the following APIs](https://console.cloud.google.com/flows/enableapi?apiid=cloudresourcemanager.googleapis.com,aiplatform.googleapis.com,cloudfunctions.googleapis.com,run.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID and project number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM1iC_MfAts1",
        "outputId": "3a899cc9-8ed8-43a3-a637-5c955831b3a6"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"$YOUR_PROJECT_ID\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oZpm-sL8f1z_"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = !gcloud projects describe {PROJECT_ID} --format=\"get(projectNumber)\"[0]\n",
        "PROJECT_NUMBER = PROJECT_NUMBER[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I6FmBV2_0fBP"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\" #\"asia-northeast3\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "#### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"$YOUR_BUCKET_NAME\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account and permissions\n",
        "\n",
        "Vertex AI Prompt optimizer requires a service account with the following permissions:\n",
        "\n",
        "-   `Vertex AI User` to call Vertex LLM API\n",
        "-   `Storage Object Admin` to read and write to your GCS bucket.\n",
        "-   `Artifact Registry Reader` to download the pipeline template from Artifact Registry.\n",
        "-   `Cloud Run Developer` to deploy function on Cloud Run.\n",
        "\n",
        "[Check out the documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts#iam-view-access-sa-gcloud) to learn how to grant those permissions to a single service account.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNRKI99_QtGR"
      },
      "source": [
        "> If you run following commands using Vertex AI Workbench, please directly run in the terminal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ssUJJqXJJHgC"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqOHg5aid6HP",
        "outputId": "fabe366c-ac55-4038-a1f8-ed9b3fdc41b9"
      },
      "outputs": [],
      "source": [
        "for role in ['aiplatform.user', 'storage.objectAdmin', 'artifactregistry.reader', 'run.developer', 'run.invoker']:\n",
        "\n",
        "    ! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "      --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "      --role=roles/{role} --condition=None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek1-iTbPjzdJ"
      },
      "source": [
        "### Set tutorial folder and workspace\n",
        "\n",
        "Set a local folder to collect and organize data and any tutorial artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BbfKRabXj3la"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path as path\n",
        "\n",
        "ROOT_PATH = path.cwd()\n",
        "TUTORIAL_PATH = ROOT_PATH / \"tutorial_case2_3\"\n",
        "BUILD_PATH = TUTORIAL_PATH / \"build_case2_3\"\n",
        "\n",
        "TUTORIAL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "BUILD_PATH.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaNdfftpXTIX"
      },
      "source": [
        "Set an associated workspace to store prompt optimization results on Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "joJPc3FmX1fk"
      },
      "outputs": [],
      "source": [
        "from etils import epath\n",
        "\n",
        "WORKSPACE_URI = epath.Path(BUCKET_URI) / \"optimization_case2_3\"\n",
        "INPUT_DATA_URI = epath.Path(WORKSPACE_URI) / \"data_case2_3\"\n",
        "\n",
        "WORKSPACE_URI.mkdir(parents=True, exist_ok=True)\n",
        "INPUT_DATA_URI.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# Tutorial\n",
        "from argparse import Namespace\n",
        "import json\n",
        "\n",
        "# General\n",
        "import logging\n",
        "from pprint import pprint\n",
        "import warnings\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from google.cloud import aiplatform\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "820DIvw1o8tB"
      },
      "source": [
        "### Libraries settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HKc4ZdUBo_SM"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxc7q4r-DFH4"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "0Y5t67f3DHNm"
      },
      "outputs": [],
      "source": [
        "INPUT_DATA_FILE_URI = \"$YOUR_FILE_PATH\"\n",
        "\n",
        "INPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / \"prompt_optimization_data\"\n",
        "INPUT_OPTIMIZATION_DATA_FILE_URI = str(\n",
        "    INPUT_DATA_URI / \"prompt_optimization_dataset.jsonl\"\n",
        ")\n",
        "OUTPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / \"optimization_jobs\"\n",
        "APD_CONTAINER_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/apd:preview_v1_0\"\n",
        ")\n",
        "CONFIG_FILE_URI = str(WORKSPACE_URI / \"config\" / \"config.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXUXmoMVVOmX",
        "outputId": "97100885-f066-491c-c83b-3bd531929b70"
      },
      "outputs": [],
      "source": [
        "print(REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "bQMc2Uwf0fBQ"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## 3. Automated prompt design with Vertex AI prompt optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmTotjRAJplw"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Load the STT dataset from a Google Cloud Storage bucket. The dataset contains the following columns:\n",
        "\n",
        "*   `audio`\n",
        "*   `target`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "LA7aG08wJtVm"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_df = pd.read_json(INPUT_DATA_FILE_URI, lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1xn-pz3v5HVK",
        "outputId": "8625c4ac-3514-485f-dd25-ec5a7af07ace"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blpKw5_YQghr",
        "outputId": "1f505dd5-0409-4993-b230-5da36f928139"
      },
      "outputs": [],
      "source": [
        "print(prompt_optimization_df['target'][6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "b8zgOTZVa6xa",
        "outputId": "72c053fa-0422-434a-8902-f9e4d8795f66"
      },
      "outputs": [],
      "source": [
        "vapo_lib.print_df_rows(prompt_optimization_df[['audio', 'target']], n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp1n1aMACzSW"
      },
      "source": [
        "### Optimize the prompt template with Vertex AI prompt optimizer with custom metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1650lf3X8xW"
      },
      "source": [
        "#### Prepare the prompt template you want to optimize\n",
        "\n",
        "A prompt consists of two key parts:\n",
        "\n",
        "* **System Instruction Template** which is a fixed part of the prompt that control or alter the model's behavior across all queries for a given task.\n",
        "\n",
        "* **Prompt Template** which is a dynamic part of the prompt that changes based on the task. Prompt template includes context, task and more. To learn more, see [components of a prompt](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies#components-of-a-prompt) in the official documentation.\n",
        "\n",
        "In this scenario, you use Vertex AI prompt optimizer to optimize a simple system instruction template. And you use some examples in the remaining prompt template for evaluating different instruction templates along the optimization process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf107f3cc99"
      },
      "source": [
        "> Having the `target` placeholder in the prompt template is optional. It represents the prompt's ground truth response in your prompt optimization dataset that you aim to optimize for your templates. If you don't have the prompt's ground truth response, remember to set the `source_model` parameter to your prompt optimizer configuration (see below) instead of adding ground truth responses. Vertex AI prompt optimizer would run your sample prompts on the source model to generate the ground truth responses for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Db8rHNC6DmtY"
      },
      "outputs": [],
      "source": [
        "SYSTEM_INSTRUCTION_TEMPLATE = \"\"\"\n",
        "Generate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Speech: {{audio}} @@@audio/wav\n",
        "Answer : {{target}}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1TCgXsrXztm"
      },
      "source": [
        "#### Prepare the prompt optimization dataset\n",
        "\n",
        "To use Vertex AI prompt optimizer, you'll need a CSV or JSONL file with labeled examples.  These examples should follow a specific naming convention. For details see [Optimize prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYV-fansHMM_"
      },
      "source": [
        "> For effective **prompt optimization**, provide a dataset of examples where your model is poor in performance when using current system instruction template. For reliable results, use 50-100 distinct samples.\n",
        "\n",
        "> In case of **prompt migration**, consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxpid3KgAkYM"
      },
      "source": [
        "#### Define and deploy your own custom optimization metric on Cloud function\n",
        "\n",
        "To optimize your prompt template using a custom optimization metric, you need to deploy a function with your own metric code on a Cloud function. To deploy a Cloud function with your own custom metric, you cover the following steps:\n",
        "\n",
        "1.   Define requirements\n",
        "2.   Write your own custom metric function code\n",
        "3.   Deploy the custom code as Cloud function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxh2e88fAnQc"
      },
      "source": [
        "##### Define requirements\n",
        "\n",
        "Set the custom metric dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "q-hUlhgBCus4"
      },
      "outputs": [],
      "source": [
        "requirements = \"\"\"\n",
        "functions-framework==3.*\n",
        "google-cloud-aiplatform\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(BUILD_PATH / \"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "1jP_ICMYrDuW"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "class OptimizationConfig(BaseModel):\n",
        "    \"\"\"\n",
        "    A comprehensive prompt optimization configuration model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Basic Configuration\n",
        "    system_instruction: str = Field(\n",
        "        ...,\n",
        "        description=\"System instructions for the target model. String. This field is required.\",\n",
        "    )\n",
        "    prompt_template: str = Field(\n",
        "        ..., description=\"Template for prompts. String. This field is required.\"\n",
        "    )\n",
        "    target_model: str = Field(\n",
        "        \"gemini-2.5-flash\",\n",
        "        description='Target model for optimization. Supported models: \"gemini-2.5-flash\", \"gemini-2.5-pro\"',\n",
        "    )\n",
        "    thinking_budget: int = Field(\n",
        "        -1,\n",
        "        description=\"Thinking budget for thinking models. -1 means auto/no thinking. Integer.\",\n",
        "    )\n",
        "    optimization_mode: str = Field(\n",
        "        \"instruction\",\n",
        "        description='Optimization mode. Supported modes: \"instruction\", \"demonstration\", \"instruction_and_demo\".',\n",
        "    )\n",
        "    project: str = Field(\n",
        "        ..., description=\"Google Cloud project ID. This field is required.\"\n",
        "    )\n",
        "\n",
        "    # Evaluation Settings\n",
        "    eval_metrics_types: List[str] = Field(\n",
        "        description='List of evaluation metrics. E.g., \"bleu\", \"rouge_l\", \"safety\".'\n",
        "    )\n",
        "    eval_metrics_weights: List[float] = Field(\n",
        "        description=\"Weights for evaluation metrics. Length must match eval_metrics_types and should sum to 1.\"\n",
        "    )\n",
        "    aggregation_type: str = Field(\n",
        "        \"weighted_sum\",\n",
        "        description='Aggregation type for metrics. Supported: \"weighted_sum\", \"weighted_average\".',\n",
        "    )\n",
        "    custom_metric_name: str = Field(\n",
        "        \"\",\n",
        "        description=\"Metric name, as defined by the key that corresponds in the dictionary returned from Cloud function. String.\",\n",
        "    )\n",
        "    custom_metric_cloud_function_name: str = Field(\n",
        "        \"\",\n",
        "        description=\"Cloud Run function name you previously deployed. String.\",\n",
        "    )\n",
        "\n",
        "    # Data and I/O Paths\n",
        "    input_data_path: str = Field(\n",
        "        ...,\n",
        "        description=\"Cloud Storage URI to input optimization data. This field is required.\",\n",
        "    )\n",
        "    output_path: str = Field(\n",
        "        ...,\n",
        "        description=\"Cloud Storage URI to save optimization results. This field is required.\",\n",
        "    )\n",
        "\n",
        "    # (Optional) Advanced Configuration\n",
        "    num_steps: int = Field(\n",
        "        10,\n",
        "        ge=10,\n",
        "        le=20,\n",
        "        description=\"Number of iterations in instruction optimization mode. Integer between 10 and 20.\",\n",
        "    )\n",
        "    num_demo_set_candidates: int = Field(\n",
        "        10,\n",
        "        ge=10,\n",
        "        le=30,\n",
        "        description=\"Number of demonstrations evaluated. Integer between 10 and 30.\",\n",
        "    )\n",
        "    demo_set_size: int = Field(\n",
        "        3,\n",
        "        ge=3,\n",
        "        le=6,\n",
        "        description=\"Number of demonstrations generated per prompt. Integer between 3 and 6.\",\n",
        "    )\n",
        "\n",
        "    # (Optional) Model Locations and QPS\n",
        "    target_model_location: str = Field(\n",
        "        \"us-central1\", description=\"Location of the target model. Default us-central1.\"\n",
        "    )\n",
        "    target_model_qps: int = Field(\n",
        "        1,\n",
        "        ge=1,\n",
        "        description=\"QPS for the target model. Integer >= 1, based on your quota.\",\n",
        "    )\n",
        "    optimizer_model_location: str = Field(\n",
        "        \"us-central1\",\n",
        "        description=\"Location of the optimizer model. Default us-central1.\",\n",
        "    )\n",
        "    optimizer_model_qps: int = Field(\n",
        "        1,\n",
        "        ge=1,\n",
        "        description=\"QPS for the optimization model. Integer >= 1, based on your quota.\",\n",
        "    )\n",
        "    source_model: str = Field(\n",
        "        \"\",\n",
        "        description=\"Google model previously used with these prompts. Not needed if providing a target column.\",\n",
        "    )\n",
        "    source_model_location: str = Field(\n",
        "        \"us-central1\", description=\"Location of the source model. Default us-central1.\"\n",
        "    )\n",
        "    source_model_qps: Optional[int] = Field(\n",
        "        None, ge=1, description=\"Optional QPS for the source model. Integer >= 1.\"\n",
        "    )\n",
        "    eval_qps: int = Field(\n",
        "        1,\n",
        "        ge=1,\n",
        "        description=\"QPS for the eval model. Integer >= 1, based on your quota.\",\n",
        "    )\n",
        "\n",
        "    # (Optional) Response, Language, and Data Handling\n",
        "    response_mime_type: str = Field(\n",
        "        \"text/plain\",\n",
        "        description=\"MIME response type from the target model. E.g., 'text/plain', 'application/json'.\",\n",
        "    )\n",
        "    response_schema: str = Field(\n",
        "        \"\", description=\"The Vertex AI Controlled Generation response schema.\"\n",
        "    )\n",
        "    language: str = Field(\n",
        "        \"English\",\n",
        "        description='Language of the system instructions. E.g., \"English\", \"Japanese\".',\n",
        "    )\n",
        "    placeholder_to_content: Dict[str, Any] = Field(\n",
        "        {},\n",
        "        description=\"Dictionary of placeholders to replace parameters in the system instruction.\",\n",
        "    )\n",
        "    data_limit: int = Field(\n",
        "        10,\n",
        "        ge=5,\n",
        "        le=100,\n",
        "        description=\"Amount of data used for validation. Integer between 5 and 100.\",\n",
        "    )\n",
        "    translation_source_field_name: str = Field(\n",
        "        \"\",\n",
        "        description=\"Field name for source text if using translation metrics (Comet, MetricX).\",\n",
        "    )\n",
        "    has_multimodal_inputs: bool = Field(\n",
        "        False, description=\"Whether the input data is multimodal.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4BoaXBlx4CGt",
        "outputId": "97fccbe0-cce8-4b57-8d61-e92c002d8c5a"
      },
      "outputs": [],
      "source": [
        "INPUT_DATA_FILE_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "dUdbgB0dqz2N"
      },
      "outputs": [],
      "source": [
        "output_path = f\"{BUCKET_URI}/optimization_results/\"\n",
        "\n",
        "# English\n",
        "# vapo_data_settings = {\n",
        "#     \"system_instruction\": SYSTEM_INSTRUCTION_TEMPLATE,\n",
        "#     \"prompt_template\": PROMPT_TEMPLATE,\n",
        "#     \"has_multimodal_inputs\": True,\n",
        "#     \"target_model\": \"gemini-2.5-flash\",\n",
        "#     \"thinking_budget\": -1,\n",
        "#     \"optimization_mode\": \"instruction\",\n",
        "#     \"eval_metrics_types\": [\"exact_match\"],\n",
        "#     \"eval_metrics_weights\": [1.0],\n",
        "#     \"aggregation_type\": \"weighted_sum\",\n",
        "#     \"input_data_path\": INPUT_DATA_FILE_URI,\n",
        "#     \"output_path\": output_path,\n",
        "#     \"project\": PROJECT_ID,\n",
        "#     \"language\": \"English\"\n",
        "# }\n",
        "\n",
        "# Mandarin\n",
        "# vapo_data_settings = {\n",
        "#     \"system_instruction\": SYSTEM_INSTRUCTION_TEMPLATE,\n",
        "#     \"prompt_template\": PROMPT_TEMPLATE,\n",
        "#     \"has_multimodal_inputs\": True,\n",
        "#     \"target_model\": \"gemini-2.5-flash\",\n",
        "#     \"thinking_budget\": -1,\n",
        "#     \"optimization_mode\": \"instruction\", \n",
        "#     \"eval_metrics_types\": [\"exact_match\"],\n",
        "#     \"eval_metrics_weights\": [1.0],\n",
        "#     \"aggregation_type\": \"weighted_sum\",\n",
        "#     \"input_data_path\": INPUT_DATA_FILE_URI,\n",
        "#     \"output_path\": output_path,\n",
        "#     \"project\": PROJECT_ID,\n",
        "#     \"language\": \"Simplified Chinese\"\n",
        "# }\n",
        "\n",
        "# Korean\n",
        "vapo_data_settings = {\n",
        "    \"system_instruction\": SYSTEM_INSTRUCTION_TEMPLATE,\n",
        "    \"prompt_template\": PROMPT_TEMPLATE,\n",
        "    \"has_multimodal_inputs\": True,\n",
        "    \"target_model\": \"gemini-2.5-flash\",\n",
        "    \"thinking_budget\": -1,\n",
        "    \"optimization_mode\": \"instruction\",\n",
        "    \"eval_metrics_types\": [\"exact_match\"],\n",
        "    \"eval_metrics_weights\": [1.0],\n",
        "    \"aggregation_type\": \"weighted_sum\",\n",
        "    \"input_data_path\": INPUT_DATA_FILE_URI,\n",
        "    \"output_path\": output_path,\n",
        "    \"project\": PROJECT_ID,\n",
        "    \"language\": \"Korean\"\n",
        "}\n",
        "\n",
        "vapo_data_config = OptimizationConfig(**vapo_data_settings)\n",
        "vapo_data_config_json = vapo_data_config.model_dump()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "UrHqhbHVrNsf"
      },
      "outputs": [],
      "source": [
        "config_path = f\"{BUCKET_URI}/config.json\"\n",
        "\n",
        "with epath.Path(config_path).open(\"w\") as config_file:\n",
        "    json.dump(vapo_data_config_json, config_file)\n",
        "config_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW0O2lZ0rQa5",
        "outputId": "cd6585a8-062c-465f-e5ce-5bab3e483ddb"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "client = vertexai.Client(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "vapo_data_run_config = {\n",
        "    \"config_path\": config_path,\n",
        "    \"wait_for_completion\": False,\n",
        "    \"service_account\": SERVICE_ACCOUNT,\n",
        "}\n",
        "print(output_path)\n",
        "\n",
        "client.prompt_optimizer.optimize(method=\"vapo\", config=vapo_data_run_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "J9VRi-0XHWup"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def format_demonstrations(demos: Any) -> List[str]:\n",
        "    \"\"\"Format demonstrations into readable strings.\"\"\"\n",
        "    if isinstance(demos, str):\n",
        "        try:\n",
        "            demos = json.loads(demos)\n",
        "        except (json.JSONDecodeError, ValueError):\n",
        "            return []\n",
        "\n",
        "    if not isinstance(demos, list):\n",
        "        return []\n",
        "\n",
        "    formatted = []\n",
        "    for demo in demos:\n",
        "        if isinstance(demo, dict):\n",
        "            demo_str = \"\\n\".join(f\"{k}: {v}\" for k, v in demo.items())\n",
        "            formatted.append(demo_str)\n",
        "        else:\n",
        "            formatted.append(str(demo))\n",
        "\n",
        "    return formatted\n",
        "\n",
        "\n",
        "def split_gcs_path(gcs_path: str) -> Tuple[str, str]:\n",
        "    \"\"\"Split GCS path into bucket name and prefix.\"\"\"\n",
        "    if not gcs_path.startswith(\"gs://\"):\n",
        "        raise ValueError(f\"Invalid GCS path. Must start with gs://: {gcs_path}\")\n",
        "\n",
        "    path = gcs_path[len(\"gs://\"):]\n",
        "    parts = path.split(\"/\", 1)\n",
        "    return parts[0], parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "\n",
        "def list_gcs_objects(gcs_path: str) -> List[str]:\n",
        "    \"\"\"List all objects under given GCS path.\"\"\"\n",
        "    bucket_name, prefix = parse_gcs_path(gcs_path)\n",
        "\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs(prefix=prefix)\n",
        "\n",
        "    return [blob.name for blob in blobs]\n",
        "\n",
        "\n",
        "def find_directories_with_files(\n",
        "    base_path: str, required_files: List[str]\n",
        ") -> List[str]:\n",
        "    \"\"\"Find directories containing all required files.\"\"\"\n",
        "    bucket_name, prefix = split_gcs_path(base_path)\n",
        "    all_paths = list_gcs_objects(base_path)\n",
        "\n",
        "    # Group files by directory\n",
        "    directories: Dict[str, set] = {}\n",
        "    for path in all_paths:\n",
        "        dir_path = \"/\".join(path.split(\"/\")[:-1])\n",
        "        filename = path.split(\"/\")[-1]\n",
        "\n",
        "        if dir_path not in directories:\n",
        "            directories[dir_path] = set()\n",
        "        directories[dir_path].add(filename)\n",
        "\n",
        "    # Find directories with all required files\n",
        "    matching_dirs = []\n",
        "    for dir_path, files in directories.items():\n",
        "        if all(req_file in files for req_file in required_files):\n",
        "            matching_dirs.append(f\"gs://{bucket_name}/{dir_path}\")\n",
        "\n",
        "    return matching_dirs\n",
        "\n",
        "def parse_gcs_path(gcs_path: str) -> Tuple[str, str]:\n",
        "    \"\"\"Parse GCS path into bucket name and prefix.\"\"\"\n",
        "    if not gcs_path.startswith(\"gs://\"):\n",
        "        raise ValueError(\"Invalid GCS path. Must start with gs://\")\n",
        "\n",
        "    path_without_prefix = gcs_path[5:]  # Remove 'gs://'\n",
        "    parts = path_without_prefix.split(\"/\", 1)\n",
        "    bucket_name = parts[0]\n",
        "    prefix = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    return bucket_name, prefix\n",
        "\n",
        "def get_best_vapo_results(\n",
        "    base_path: str, metric_name: Optional[str] = None\n",
        ") -> Tuple[str, List[str]]:\n",
        "    \"\"\"Get the best system instruction and demonstrations across all VAPO runs.\"\"\"\n",
        "    # Find all valid runs\n",
        "    required_files = [\"eval_results.json\", \"templates.json\"]\n",
        "    runs = find_directories_with_files(base_path, required_files)\n",
        "\n",
        "    if not runs:\n",
        "        raise ValueError(f\"No valid runs found in {base_path}\")\n",
        "\n",
        "    best_score = float(\"-inf\")\n",
        "    best_instruction = \"\"\n",
        "    best_demonstrations: List[str] = []\n",
        "\n",
        "    for run_path in runs:\n",
        "        try:\n",
        "            # Check main templates.json first\n",
        "            templates_path = f\"{run_path}/templates.json\"\n",
        "            with epath.Path(templates_path).open(\"r\") as f:\n",
        "                templates_data = json.load(f)\n",
        "\n",
        "            if templates_data:\n",
        "                df = pd.json_normalize(templates_data)\n",
        "\n",
        "                # Find metric column\n",
        "                metric_columns = [\n",
        "                    col for col in df.columns\n",
        "                    if \"metric\" in col and \"mean\" in col\n",
        "                ]\n",
        "\n",
        "                if metric_columns:\n",
        "                    # Select appropriate metric\n",
        "                    if metric_name:\n",
        "                        metric_col = next(\n",
        "                            (col for col in metric_columns if metric_name in col),\n",
        "                            None\n",
        "                        )\n",
        "                    else:\n",
        "                        composite_cols = [\n",
        "                            col for col in metric_columns\n",
        "                            if \"composite_metric\" in col\n",
        "                        ]\n",
        "                        metric_col = (\n",
        "                            composite_cols[0] if composite_cols else metric_columns[0]\n",
        "                        )\n",
        "\n",
        "                    if metric_col and metric_col in df.columns:\n",
        "                        best_idx = df[metric_col].argmax()\n",
        "                        score = float(df.iloc[best_idx][metric_col])\n",
        "\n",
        "                        if score > best_score:\n",
        "                            best_score = score\n",
        "                            best_row = df.iloc[best_idx]\n",
        "\n",
        "                            # Extract instruction if present\n",
        "                            if \"prompt\" in best_row or \"instruction\" in best_row:\n",
        "                                instruction = best_row.get(\n",
        "                                    \"prompt\", best_row.get(\"instruction\", \"\")\n",
        "                                )\n",
        "                                if instruction:\n",
        "                                    instruction = instruction.replace(\n",
        "                                        \"store('answer', llm())\", \"{{llm()}}\"\n",
        "                                    )\n",
        "                                    best_instruction = instruction\n",
        "\n",
        "                            # Extract demonstrations if present\n",
        "                            if \"demonstrations\" in best_row or \"demo_set\" in best_row:\n",
        "                                demos = best_row.get(\n",
        "                                    \"demonstrations\", best_row.get(\"demo_set\", [])\n",
        "                                )\n",
        "                                best_demonstrations = format_demonstrations(demos)\n",
        "\n",
        "            # Check instruction-specific optimization\n",
        "            instruction_path = f\"{run_path}/instruction/templates.json\"\n",
        "            try:\n",
        "                with epath.Path(instruction_path).open(\"r\") as f:\n",
        "                    instruction_data = json.load(f)\n",
        "\n",
        "                if instruction_data:\n",
        "                    inst_df = pd.json_normalize(instruction_data)\n",
        "                    metric_columns = [\n",
        "                        col for col in inst_df.columns\n",
        "                        if \"metric\" in col and \"mean\" in col\n",
        "                    ]\n",
        "\n",
        "                    if metric_columns:\n",
        "                        if metric_name:\n",
        "                            metric_col = next(\n",
        "                                (col for col in metric_columns if metric_name in col),\n",
        "                                None,\n",
        "                            )\n",
        "                        else:\n",
        "                            composite_cols = [\n",
        "                                col for col in metric_columns\n",
        "                                if \"composite_metric\" in col\n",
        "                            ]\n",
        "                            metric_col = (\n",
        "                                composite_cols[0] if composite_cols else metric_columns[0]\n",
        "                            )\n",
        "\n",
        "                        if metric_col and metric_col in inst_df.columns:\n",
        "                            inst_best_idx = inst_df[metric_col].argmax()\n",
        "                            inst_score = float(inst_df.iloc[inst_best_idx][metric_col])\n",
        "\n",
        "                            if inst_score > best_score:\n",
        "                                best_score = inst_score\n",
        "                                best_row = inst_df.iloc[inst_best_idx]\n",
        "\n",
        "                                instruction = best_row.get(\n",
        "                                    \"prompt\", best_row.get(\"instruction\", \"\")\n",
        "                                )\n",
        "                                if instruction:\n",
        "                                    instruction = instruction.replace(\n",
        "                                        \"store('answer', llm())\", \"{{llm()}}\"\n",
        "                                    )\n",
        "                                    best_instruction = instruction\n",
        "                                # In instruction-only mode, there might not be demonstrations\n",
        "                                if \"demonstrations\" not in best_row and \"demo_set\" not in best_row:\n",
        "                                    best_demonstrations = []\n",
        "            except FileNotFoundError:\n",
        "                pass\n",
        "\n",
        "            # Check demonstration-specific optimization\n",
        "            demo_path = f\"{run_path}/demonstration/templates.json\"\n",
        "            try:\n",
        "                with epath.Path(demo_path).open(\"r\") as f:\n",
        "                    demo_data = json.load(f)\n",
        "\n",
        "                if demo_data:\n",
        "                    demo_df = pd.json_normalize(demo_data)\n",
        "                    metric_columns = [\n",
        "                        col for col in demo_df.columns\n",
        "                        if \"metric\" in col and \"mean\" in col\n",
        "                    ]\n",
        "\n",
        "                    if metric_columns:\n",
        "                        if metric_name:\n",
        "                            metric_col = next(\n",
        "                                (col for col in metric_columns if metric_name in col),\n",
        "                                None,\n",
        "                            )\n",
        "                        else:\n",
        "                            composite_cols = [\n",
        "                                col for col in metric_columns\n",
        "                                if \"composite_metric\" in col\n",
        "                            ]\n",
        "                            metric_col = (\n",
        "                                composite_cols[0] if composite_cols else metric_columns[0]\n",
        "                            )\n",
        "\n",
        "                        if metric_col and metric_col in demo_df.columns:\n",
        "                            demo_best_idx = demo_df[metric_col].argmax()\n",
        "                            demo_score = float(demo_df.iloc[demo_best_idx][metric_col])\n",
        "\n",
        "                            if demo_score > best_score:\n",
        "                                best_score = demo_score\n",
        "                                best_row = demo_df.iloc[demo_best_idx]\n",
        "\n",
        "                                demos = best_row.get(\n",
        "                                    \"demonstrations\", best_row.get(\"demo_set\", [])\n",
        "                                )\n",
        "                                best_demonstrations = format_demonstrations(demos)\n",
        "                                # In demo-only mode, there might not be an instruction\n",
        "                                if \"prompt\" not in best_row and \"instruction\" not in best_row:\n",
        "                                    best_instruction = \"\"\n",
        "                                else:\n",
        "                                    instruction = best_row.get(\n",
        "                                        \"prompt\", best_row.get(\"instruction\", \"\")\n",
        "                                    )\n",
        "                                    if instruction:\n",
        "                                        instruction = instruction.replace(\n",
        "                                            \"store('answer', llm())\", \"{{llm()}}\"\n",
        "                                        )\n",
        "                                        best_instruction = instruction\n",
        "            except (FileNotFoundError, json.JSONDecodeError):\n",
        "                pass\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error processing run {run_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if best_score == float(\"-inf\"):\n",
        "        raise ValueError(\"Could not find any valid results\")\n",
        "\n",
        "    return best_instruction, best_demonstrations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VZKRP4UrNui",
        "outputId": "b832250a-cd98-442b-96bf-ed93d12479f0"
      },
      "outputs": [],
      "source": [
        "best_instruction, _ = get_best_vapo_results(output_path)\n",
        "print(\"The optimized instruction is:\\n\", best_instruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayx8IkR3qtcw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## 4. Clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRY_3wh1GVNm"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "delete_job = False\n",
        "delete_tutorial = False\n",
        "\n",
        "if delete_bucket:\n",
        "    ! gsutil rm -r {BUCKET_URI}\n",
        "\n",
        "if delete_tutorial:\n",
        "    import shutil\n",
        "\n",
        "    shutil.rmtree(str(TUTORIAL_PATH))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2_3_vapo_transcription.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
