{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g65MmV_HKi7I"
      },
      "source": [
        "## VAPO Custom metric Example 2-3\n",
        "\n",
        "- [Original Reference](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk_custom_metric.ipynb) from [Ivan Nardini](https://github.com/inardini)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HKyj5KwYePX"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This notebook demonstrates how to leverage Vertex AI prompt optimizer to optimize a simple prompt for a Gemini model using your own metric. The goal is to use Vertex AI prompt optimizer to find a new prompt template that generates better responses based on your own optimization metric.\n",
        "\n",
        "This tutorial uses the following Google Cloud services and resources:\n",
        "\n",
        "- Generative AI on Vertex AI\n",
        "- Vertex AI prompt optimizer\n",
        "- Vertex AI Gen AI evaluation\n",
        "- Vertex AI Custom job\n",
        "- Cloud Run\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "1. Define the prompt template you want to optimize.\n",
        "2. Prepare the prompt optimization dataset.\n",
        "3. Define and deploy your own custom evaluation metric on Cloud function.\n",
        "4. Set optimization mode and steps.\n",
        "5. Run the automatic prompt optimization job.\n",
        "6. Collect the best prompt template and eval metric.\n",
        "7. Validate the best prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## 2. Before you start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enFkr5SXo36F",
        "outputId": "90324844-b65c-4b70-d7c3-73803a2460b8"
      },
      "outputs": [],
      "source": [
        "# %pip install --quiet 'evaluate' 'jiwer'\n",
        "%pip install --upgrade --quiet 'google-cloud-aiplatform[evaluation]'\n",
        "%pip install --upgrade --quiet 'plotly' 'asyncio' 'tqdm' 'tenacity' 'etils' 'importlib_resources' 'fsspec' 'gcsfs' 'nbformat>=4.2.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "RrKXzTVMDKb4",
        "outputId": "02e7fd11-e920-4c75-8868-035774f2df4e"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py\n",
        "import vapo_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55e2195ce2d",
        "outputId": "f8298a23-f47a-4d36-b620-ccf5c20b3452"
      },
      "outputs": [],
      "source": [
        "! mkdir -p ./tutorial/utils && wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py -P ./tutorial/utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyKGtVQjgx13",
        "outputId": "134e4e5b-4963-40f3-dbc0-87ec36046976"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    try:\n",
        "        import google.auth\n",
        "        import google.auth.transport.requests\n",
        "        from google.colab import auth\n",
        "\n",
        "        auth.authenticate_user()\n",
        "        creds, project = google.auth.default()\n",
        "        authentication = google.auth.transport.requests.Request()\n",
        "        if creds.token:\n",
        "            print(\"Authentication successful.\")\n",
        "        else:\n",
        "            print(\"Authentication successful, but no token was returned.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Colab authentication: {e}\")\n",
        "\n",
        "! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the following APIs](https://console.cloud.google.com/flows/enableapi?apiid=cloudresourcemanager.googleapis.com,aiplatform.googleapis.com,cloudfunctions.googleapis.com,run.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID and project number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM1iC_MfAts1",
        "outputId": "fa74da6c-0ec1-4905-88b7-d851bef3939a"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"$YOUR_PROJECT_ID\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZpm-sL8f1z_"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = !gcloud projects describe {PROJECT_ID} --format=\"get(projectNumber)\"[0]\n",
        "PROJECT_NUMBER = PROJECT_NUMBER[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6FmBV2_0fBP"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\" #\"asia-northeast3\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "#### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"$YOUR_BUCKET_NAME\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account and permissions\n",
        "\n",
        "Vertex AI Prompt optimizer requires a service account with the following permissions:\n",
        "\n",
        "-   `Vertex AI User` to call Vertex LLM API\n",
        "-   `Storage Object Admin` to read and write to your GCS bucket.\n",
        "-   `Artifact Registry Reader` to download the pipeline template from Artifact Registry.\n",
        "-   `Cloud Run Developer` to deploy function on Cloud Run.\n",
        "\n",
        "[Check out the documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts#iam-view-access-sa-gcloud) to learn how to grant those permissions to a single service account.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNRKI99_QtGR"
      },
      "source": [
        "> If you run following commands using Vertex AI Workbench, please directly run in the terminal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssUJJqXJJHgC"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqOHg5aid6HP",
        "outputId": "3f677af4-7478-40e3-bb2d-af23cd7b76d6"
      },
      "outputs": [],
      "source": [
        "for role in ['aiplatform.user', 'storage.objectAdmin', 'artifactregistry.reader', 'run.developer', 'run.invoker']:\n",
        "\n",
        "    ! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "      --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "      --role=roles/{role} --condition=None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek1-iTbPjzdJ"
      },
      "source": [
        "### Set tutorial folder and workspace\n",
        "\n",
        "Set a local folder to collect and organize data and any tutorial artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbfKRabXj3la"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path as path\n",
        "\n",
        "ROOT_PATH = path.cwd()\n",
        "TUTORIAL_PATH = ROOT_PATH / \"tutorial_case2_3_custom\"\n",
        "BUILD_PATH = TUTORIAL_PATH / \"build_case2_3_custom\"\n",
        "\n",
        "TUTORIAL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "BUILD_PATH.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaNdfftpXTIX"
      },
      "source": [
        "Set an associated workspace to store prompt optimization results on Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joJPc3FmX1fk"
      },
      "outputs": [],
      "source": [
        "from etils import epath\n",
        "\n",
        "WORKSPACE_URI = epath.Path(BUCKET_URI) / \"optimization_case2_3_custom\"\n",
        "INPUT_DATA_URI = epath.Path(WORKSPACE_URI) / \"data_case2_3_custom\"\n",
        "\n",
        "WORKSPACE_URI.mkdir(parents=True, exist_ok=True)\n",
        "INPUT_DATA_URI.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# Tutorial\n",
        "from argparse import Namespace\n",
        "import json\n",
        "\n",
        "# General\n",
        "import logging\n",
        "from pprint import pprint\n",
        "import warnings\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from google.cloud import aiplatform\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "820DIvw1o8tB"
      },
      "source": [
        "### Libraries settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKc4ZdUBo_SM"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxc7q4r-DFH4"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y5t67f3DHNm"
      },
      "outputs": [],
      "source": [
        "INPUT_DATA_FILE_URI = \"$YOUR_DATA_URI\"\n",
        "\n",
        "\n",
        "INPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / \"prompt_optimization_data\"\n",
        "INPUT_OPTIMIZATION_DATA_FILE_URI = str(\n",
        "    INPUT_DATA_URI / \"prompt_optimization_dataset.jsonl\"\n",
        ")\n",
        "OUTPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / \"optimization_jobs\"\n",
        "APD_CONTAINER_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/apd:preview_v1_0\"\n",
        ")\n",
        "CONFIG_FILE_URI = str(WORKSPACE_URI / \"config\" / \"config.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXUXmoMVVOmX",
        "outputId": "861b84e2-1d35-44ee-de33-a1efb69f0aaa"
      },
      "outputs": [],
      "source": [
        "print(REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQMc2Uwf0fBQ"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## 3. Automated prompt design with Vertex AI prompt optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmTotjRAJplw"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Load the cooking question-answer dataset from a Google Cloud Storage bucket. The dataset contains the following columns:\n",
        "\n",
        "*   `user_question`: The cooking question posed by the user to the AI cooking assistant.\n",
        "*   `context`: Relevant information retrieved to answer the user's question.\n",
        "*   `prompt`: The content fed to the language model to generate an answer.\n",
        "*   `answer`: The generated answer from the language model.\n",
        "*   `reference`: The ground truth answer—the ideal response the user expects from the AI cooking assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA7aG08wJtVm"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_df = pd.read_json(INPUT_DATA_FILE_URI, lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1xn-pz3v5HVK",
        "outputId": "6779662f-b4a9-43a2-9503-d85d41a2acf0"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "b8zgOTZVa6xa",
        "outputId": "c83950b5-5db9-48cd-ac15-23971a3597d1"
      },
      "outputs": [],
      "source": [
        "vapo_lib.print_df_rows(prompt_optimization_df[['target', 'audio']], n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp1n1aMACzSW"
      },
      "source": [
        "### Optimize the prompt template with Vertex AI prompt optimizer with custom metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1650lf3X8xW"
      },
      "source": [
        "#### Prepare the prompt template you want to optimize\n",
        "\n",
        "A prompt consists of two key parts:\n",
        "\n",
        "* **System Instruction Template** which is a fixed part of the prompt that control or alter the model's behavior across all queries for a given task.\n",
        "\n",
        "* **Prompt Template** which is a dynamic part of the prompt that changes based on the task. Prompt template includes context, task and more. To learn more, see [components of a prompt](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies#components-of-a-prompt) in the official documentation.\n",
        "\n",
        "In this scenario, you use Vertex AI prompt optimizer to optimize a simple system instruction template. And you use some examples in the remaining prompt template for evaluating different instruction templates along the optimization process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf107f3cc99"
      },
      "source": [
        "> Having the `target` placeholder in the prompt template is optional. It represents the prompt's ground truth response in your prompt optimization dataset that you aim to optimize for your templates. If you don't have the prompt's ground truth response, remember to set the `source_model` parameter to your prompt optimizer configuration (see below) instead of adding ground truth responses. Vertex AI prompt optimizer would run your sample prompts on the source model to generate the ground truth responses for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db8rHNC6DmtY"
      },
      "outputs": [],
      "source": [
        "SYSTEM_INSTRUCTION_TEMPLATE = \"\"\"\n",
        "Generate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Speech: {{audio}} @@@audio/wav\n",
        "Answer : {{target}}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1TCgXsrXztm"
      },
      "source": [
        "#### Prepare the prompt optimization dataset\n",
        "\n",
        "To use Vertex AI prompt optimizer, you'll need a CSV or JSONL file with labeled examples.  These examples should follow a specific naming convention. For details see [Optimize prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYV-fansHMM_"
      },
      "source": [
        "> For effective **prompt optimization**, provide a dataset of examples where your model is poor in performance when using current system instruction template. For reliable results, use 50-100 distinct samples.\n",
        "\n",
        "> In case of **prompt migration**, consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF3XY_d_yB-K"
      },
      "source": [
        "#### Upload samples to bucket\n",
        "\n",
        "Once you prepare your prompt optimization dataset, you can upload them on Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcPzAM6kMqUD",
        "outputId": "27c1379f-57e4-45b0-f5b1-33215d7e4f3e"
      },
      "outputs": [],
      "source": [
        "print(INPUT_OPTIMIZATION_DATA_FILE_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "155paLgGUXOm"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_df.to_json(\n",
        "    INPUT_OPTIMIZATION_DATA_FILE_URI, orient=\"records\", lines=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxpid3KgAkYM"
      },
      "source": [
        "#### Define and deploy your own custom optimization metric on Cloud function\n",
        "\n",
        "To optimize your prompt template using a custom optimization metric, you need to deploy a function with your own metric code on a Cloud function. To deploy a Cloud function with your own custom metric, you cover the following steps:\n",
        "\n",
        "1.   Define requirements\n",
        "2.   Write your own custom metric function code\n",
        "3.   Deploy the custom code as Cloud function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxh2e88fAnQc"
      },
      "source": [
        "##### Define requirements\n",
        "\n",
        "Set the custom metric dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-hUlhgBCus4"
      },
      "outputs": [],
      "source": [
        "requirements = \"\"\"\n",
        "functions-framework==3.*\n",
        "google-cloud-aiplatform\n",
        "evaluate\n",
        "jiwer\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(BUILD_PATH / \"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_EFZEBeEy48"
      },
      "source": [
        "##### Write your own custom metric function\n",
        "\n",
        "Define the module which contains your own custom metric function definition.\n",
        "\n",
        "In this case, you have a custom evaluation metric to evaluate the user engagement and personalization. The custom evaluation metric is defined using the `evaluate_engagement_personalization_fn`.\n",
        "\n",
        "The function leverages \"gemini-2.0-flash\" to act as an \"auto-rater\". It sends a prompt to the auto-rater, receives a score (1-5), and an explanation, then returns these as a dictionary containing two fields: the custom metric's score (as you defined it) and an explanation of how this metric helps optimize the prompt template.\n",
        "\n",
        "You use the `main` function to deploy the `evaluate_engagement_personalization_fn` function as a Cloud Function, receiving a question, response, and a target response as input and returning the auto-rater's evaluation.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wGoVQNCMbxe"
      },
      "outputs": [],
      "source": [
        "custom_metric_function_code = '''\n",
        "\"\"\"\n",
        "This module contains the custom evaluation metric definition to optimize a prompt template with Vertex AI prompt optimizer\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict\n",
        "from vertexai.generative_models import (\n",
        "  GenerationConfig,\n",
        "  GenerativeModel\n",
        ")\n",
        "\n",
        "import json\n",
        "import functions_framework\n",
        "import evaluate\n",
        "import string\n",
        "\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "cer_metric = evaluate.load(\"cer\")\n",
        "autorater = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    try:\n",
        "      punctuations = string.punctuation\n",
        "      translator = str.maketrans('', '', punctuations)\n",
        "      return text.translate(translator)\n",
        "    except:\n",
        "      print(f\"## ERROR when removing punc : {text}\")\n",
        "      return text\n",
        "\n",
        "def calculate_weighted_score(inference_refined: str, reference_refined: str) -> float:\n",
        "  wer_score = wer_metric.compute(predictions=[inference_refined], references=[reference_refined])\n",
        "  cer_score = cer_metric.compute(predictions=[inference_refined], references=[reference_refined])\n",
        "\n",
        "  final_score = 3 - (wer_score + 2*cer_score)\n",
        "\n",
        "  return final_score\n",
        "\n",
        "\n",
        "def get_explanation(inference_refined: str, reference_refined: str) -> str:\n",
        "  prompt = f\"\"\"Evaluate the STT (Speech-to-Text) performance based on the items provided below.\n",
        "\n",
        "  inference: {inference_refined}\n",
        "  reference: {reference_refined}\n",
        "\n",
        "  Compare the inference text against the reference text and identify all errors (e.g., substitutions, deletions, insertions) in the inference.\n",
        "\n",
        "  \"\"\"\n",
        "  response = autorater.generate_content(prompt)\n",
        "  explanation = response.text\n",
        "\n",
        "  return explanation\n",
        "\n",
        "\n",
        "# Define custom evaluation criteria\n",
        "def evaluate_engagement_personalization_fn(final_score: float, explanation:str) -> Dict[str, str]:\n",
        "  return {\n",
        "      \"custom_engagement_personalization_score\": final_score,\n",
        "      \"explanation\": explanation,\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "# Register an HTTP function with the Functions Framework\n",
        "@functions_framework.http\n",
        "def main(request):\n",
        "  default_return = {\"custom_engagement_personalization_score\": 0, \"explanation\": \"Wrong format in somewhere\"}\n",
        "  request_json = request.get_json(silent=True)\n",
        "  if not request_json:\n",
        "    raise ValueError('Cannot find request json.')\n",
        "\n",
        "  try:\n",
        "    inference_refined = remove_punctuation(request_json['response'])\n",
        "    reference_refined = remove_punctuation(request_json['target'])\n",
        "\n",
        "    final_score = calculate_weighted_score(inference_refined, reference_refined)\n",
        "    explanation = get_explanation(inference_refined, reference_refined)\n",
        "\n",
        "    get_evaluation_result = evaluate_engagement_personalization_fn(final_score, explanation)\n",
        "    return json.dumps(get_evaluation_result)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"##Exception##\")\n",
        "    print(e)\n",
        "    print(\"##Response##\")\n",
        "    print(response)\n",
        "\n",
        "    return json.dumps(default_return)\n",
        "\n",
        "'''\n",
        "\n",
        "with open(BUILD_PATH / \"main.py\", \"w\") as f:\n",
        "    f.write(custom_metric_function_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7R0LDZMCPnL"
      },
      "source": [
        "##### Deploy the custom metric as a Cloud Function\n",
        "\n",
        "Use gcloud command line to deploy a Cloud function. To learn more, check out [Deploy a Python service to Cloud Run](https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-python-service) quickstart.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwBZGvkLCizs",
        "outputId": "45d701eb-fc0c-4286-e16a-b6e6369f5a23"
      },
      "outputs": [],
      "source": [
        "!gcloud functions deploy 'custom_metric_case_2_3_transcription' \\\n",
        " --gen2 \\\n",
        " --runtime=\"python310\" \\\n",
        " --source={str(BUILD_PATH)} \\\n",
        " --entry-point=main \\\n",
        " --trigger-http \\\n",
        " --timeout=3600 \\\n",
        " --memory=2Gb \\\n",
        " --concurrency=6 \\\n",
        " --min-instances=6 \\\n",
        " --project {PROJECT_ID} \\\n",
        " --region={REGION} \\\n",
        " --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoEypczSwAGK"
      },
      "source": [
        "##### Test your custom evaluation metric\n",
        "\n",
        "After you deploy your  custom evaluation metric as Cloud function, submit a request to validate the output of the custom evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA_6GO8BOK_h",
        "outputId": "b9b03fe5-5bea-4ac1-9708-0b9e4abc7220"
      },
      "outputs": [],
      "source": [
        "! gcloud functions describe 'custom_metric_case_2_3_transcription' --gen2 --region {REGION} --format=\"value(url)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXOWYp2MwEsA"
      },
      "outputs": [],
      "source": [
        "custom_evaluator_function_uri = ! gcloud functions describe 'custom_metric_case_2_3_transcription' --gen2 --region {REGION} --format=\"value(url)\"\n",
        "custom_evaluator_function_uri = custom_evaluator_function_uri[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbXhK3qnL1FW",
        "outputId": "d4be11a3-25de-4598-b02d-7a70c2c6c4cf"
      },
      "outputs": [],
      "source": [
        "print(custom_evaluator_function_uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JMeIyx0DHnc",
        "outputId": "bf538bff-ee35-4b64-cdcd-f6058b32d5dd"
      },
      "outputs": [],
      "source": [
        "dummy_token = 'ABC' # get_auth_token()\n",
        "#     \"Authorization\": f\"Bearer {dummy_token}\",\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "# json_data = {\n",
        "#     \"response\": \"Hey!!\",\n",
        "#     \"target\": \"Hey\",\n",
        "# }\n",
        "\n",
        "\n",
        "json_data = {\n",
        "    \"response\": \"가나안에는 큰 숲이 없었기 때문에 나무가 무척 비쌌다\",\n",
        "    \"target\": \"기니안에는 큰 숲이 없었기 때문에 나무가 무척 비쌌다\",\n",
        "}\n",
        "\n",
        "response = requests.post(custom_evaluator_function_uri, headers=headers, json=json_data, timeout=70) #.json()\n",
        "pprint(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKo7t7BIvWUT",
        "outputId": "40a29aab-4524-4aae-bced-ac2646a3fa2a"
      },
      "outputs": [],
      "source": [
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5RD0l2xX-FI"
      },
      "source": [
        "#### Configure optimization settings\n",
        "\n",
        "Vertex AI prompt optimizer lets you control the optimization process by specifying what to optimize (instructions only, demonstrations only, or both), providing a system instruction and prompt template, and selecting the target model.  You can optionally refine the optimization with some advanced settings like its duration and the number of optimization iterations it runs, which models the Vertex AI prompt optimizer uses, and other parameters to control the structure and content of prompts. Below you have some common and recommended default configurations.\n",
        "\n",
        "In this scenario, you set two additional parameters:\n",
        "\n",
        "* `custom_metric_name` parameter which lets you pass your own custom metric to optimizer the prompt template.\n",
        "\n",
        "* `custom_metric_cloud_function_name` parameter which indicates the Cloud function to call for collecting custom function evaluation metric output.\n",
        "\n",
        "For more advanced control, you can learn and explore more about all the parameters and how to best use them in the [detailed documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFHutXhgeqRx"
      },
      "outputs": [],
      "source": [
        "## Important setting : response_mime_type=\"application/json\"\n",
        "\n",
        "PROMPT_OPTIMIZATION_JOB = \"auto-prompt-design-job-\" + vapo_lib.get_id()\n",
        "OUTPUT_OPTIMIZATION_RUN_URI = str(\n",
        "    OUTPUT_OPTIMIZATION_DATA_URI / PROMPT_OPTIMIZATION_JOB\n",
        ")\n",
        "\n",
        "args = Namespace(\n",
        "    # Basic configuration\n",
        "    system_instruction=SYSTEM_INSTRUCTION_TEMPLATE,  # System instructions for the target model. String.\n",
        "    prompt_template=PROMPT_TEMPLATE,  # Template for prompts,  String.\n",
        "    target_model=\"gemini-2.5-flash\",  # Target model for optimization. String. Supported models: \"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-2.0-flash-lite-001\", \"gemini-2.0-flash-001\"\n",
        "    thinking_budget=-1,  # Thinking budget for thinking models. -1 means no thinking for non-thinking models and auto thinking for thinking models. Integer.\n",
        "    optimization_mode=\"instruction_and_demo\",  # Optimization mode. String. Supported modes: \"instruction\", \"demonstration\", \"instruction_and_demo\"\n",
        "    custom_metric_name=\"custom_engagement_personalization_score\",  # Metric name, as defined by the key that corresponds in the dictionary returned from Cloud function. String.\n",
        "    custom_metric_cloud_function_name=\"custom_metric_case_2_3_transcription\",  # Cloud Run function name you previously deployed. String.\n",
        "    eval_metrics_types=[\n",
        "        \"custom_metric\",\n",
        "    ],  # List of evaluation metrics. List of strings. Supported metrics: \"bleu\", \"coherence\", \"comet\", \"exact_match\", \"fluency\", \"groundedness\", \"metricx\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"text_quality\", \"verbosity\", \"tool_call_valid\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\"\n",
        "    eval_metrics_weights=[\n",
        "        1.0\n",
        "    ],  # Weights for evaluation metrics. List of floats.  Length must match eval_metrics_types.  Should sum to 1.\n",
        "    aggregation_type=\"weighted_sum\",  # Aggregation type for evaluation metrics. String. Supported aggregation types: \"weighted_sum\", \"weighted_average\"\n",
        "    input_data_path=INPUT_OPTIMIZATION_DATA_FILE_URI,  # Cloud Storage URI to input optimization data. String.\n",
        "    output_path=OUTPUT_OPTIMIZATION_RUN_URI,  # Cloud Storage URI to save optimization results. String.\n",
        "    project=PROJECT_ID,  # Google Cloud project ID. String.\n",
        "    # (Optional) Advanced configuration\n",
        "    num_steps=10,  # Number of iterations in instruction optimization mode. Integer between 10 and 20.\n",
        "    num_demo_set_candidates=10,  # Number of demonstrations evaluated in instruction and instruction_and_demo mode. Integer between 10 and 30.\n",
        "    demo_set_size=3,  # Number of demonstrations generated per prompt. Integer between 3 and 6.\n",
        "    target_model_location=\"us-central1\", # Location of the target model. String. Default us-central1.\n",
        "    optimizer_model_location=\"us-central1\", # Location of the optimizer model. String. Default us-central1.\n",
        "    source_model=\"\",  # Google model that the system instructions and prompts were previously used with. String. Not needed if you provide target column.\n",
        "    source_model_location=\"\",  # Location of the source model. String. Default us-central1. Not needed if you provide target column.\n",
        "    target_model_qps=1,  # The queries per second (QPS) sent to the target model. Integer greater or equal than 1 depending on your quota.\n",
        "    optimizer_model_qps=1,  # The queries per second (QPS) sent to the optimization model. Integer greater or equal than 1 depending on your quota.\n",
        "    eval_qps=1,  # The queries per second (QPS) sent to the eval model. Integer greater or equal than 1 depending on your quota.\n",
        "    source_model_qps=\"\",  # The queries per second (QPS) sent to the source model. Integer greater or equal than 1 depending on your quota.\n",
        "    response_mime_type=\"text/plain\",  # MIME response type that the target model uses. String. Supported response: text/plain, text/x.enum, application/json.\n",
        "    response_schema=\"\",  # The Vertex AI's Controlled Generation response schema that the target model uses to generate answers. String.\n",
        "    language=\"Korean\",  # Language of the system instructions. String. Supported languages: \"English\", \"French\", \"German\", \"Hebrew\", \"Hindi\", \"Italian\", \"Japanese\", \"Korean\", \"Portuguese\", \"Simplified Chinese\", \"Spanish\", \"Traditional Chinese\"\n",
        "    placeholder_to_content=json.loads(\n",
        "        \"{}\"\n",
        "    ),  # Placeholder to replace any parameter in the system instruction. Dict.\n",
        "    data_limit=10,  # Amount of data used for validation. Integer between 5 and 100.\n",
        "    translation_source_field_name=\"\",  # Fill in with the corresponding field name of the source text in the data if translation metrics like Comet or MetricX are selected. Otherwise, leave it as empty.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd_uzQYQx6L7"
      },
      "source": [
        "#### Upload Vertex AI prompt optimizer Cloud Storage\n",
        "\n",
        "After you define Vertex AI prompt optimizer configuration, you upload them on Cloud Storage bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqiv8ApR_SAM"
      },
      "outputs": [],
      "source": [
        "args = vars(args)\n",
        "\n",
        "with epath.Path(CONFIG_FILE_URI).open(\"w\") as config_file:\n",
        "    json.dump(args, config_file)\n",
        "config_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqgBT8hYAle"
      },
      "source": [
        "#### Run the automatic prompt optimization job\n",
        "\n",
        "Now you are ready to run your first Vertex AI prompt optimizer job using the Vertex AI SDK for Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlNQ_UrFH9ne"
      },
      "source": [
        "> This prompt optimization job requires ~ 40 minutes to run.\n",
        "\n",
        "> Be sure you have provisioned enough queries per minute (QPM) quota implementing the recommended QPM for each model. If you configure the Vertex AI prompt optimizer with a QPM that is higher than the QPM than you have access to, the job might fail. [Check out](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer#before-you-begin) the documentation to know more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtPnvKIpUQ3q"
      },
      "outputs": [],
      "source": [
        "WORKER_POOL_SPECS = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": \"n1-standard-4\",\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": APD_CONTAINER_URI,\n",
        "            \"args\": [\"--config=\" + CONFIG_FILE_URI],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "custom_job = aiplatform.CustomJob(\n",
        "    display_name=PROMPT_OPTIMIZATION_JOB,\n",
        "    worker_pool_specs=WORKER_POOL_SPECS,\n",
        ")\n",
        "\n",
        "custom_job.submit(service_account=SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqfRaJ5iUVpP",
        "outputId": "13f8779f-0702-4d9b-e441-bfe92e3b2c2e"
      },
      "outputs": [],
      "source": [
        "custom_job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YwwKBhtJ4ut"
      },
      "source": [
        "### Collect and display the optimization results\n",
        "\n",
        "Vertex AI prompt optimizer returns both optimized templates and evaluation results for either instruction, or demostrations, or both depending on the optimization mode you define as JSONL files on Cloud Storage bucket. Those results help you understand the optimization process.\n",
        "\n",
        "In this case, you want to collect the optimized templates and evaluation results for the system instruction.\n",
        "\n",
        "Below you use a helper function to display those results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovqCR2u7QTZv",
        "outputId": "44d76b08-dd96-49e3-8013-2131e5c91325"
      },
      "outputs": [],
      "source": [
        "print(OUTPUT_OPTIMIZATION_RUN_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765,
          "referenced_widgets": [
            "edd3f054bb39449c8a981a6ab099b39b",
            "bb1594a3d51b46e892c69a75b10f3d06",
            "2efa849690394a7eba8b6830d906ba1d",
            "63b6be1599284ce6896d7328237de667",
            "901bca8937324f3f800dd1158b110dc4",
            "446a009355a849c2996fe9889314dc31",
            "d8eeefe199164a04b59576ea8a3fba21",
            "d575c6c9100446cc87a85c4b92566f0d",
            "0d83c3c05909493daa3e6342c3ae1790",
            "ae946cfb0a194d6785150c21ad70f731",
            "c4b51a19bcbf48b2a99139d46dd77e9e",
            "ac9194eab40a4a0d877078eca8028ee0",
            "9848bc9f3df049e6921e94b18b207848",
            "c0bb0a7a7181424586b5439eff8a6428",
            "e7d9c10325fa4b8ca7762a281d31f46f",
            "4a37991304204a369f32b5bf0621452d"
          ]
        },
        "id": "-6ohF0LJIhvU",
        "outputId": "2c8da062-5c79-4874-e6a0-2ff67799910e"
      },
      "outputs": [],
      "source": [
        "results_ui = vapo_lib.ResultsUI(OUTPUT_OPTIMIZATION_RUN_URI)\n",
        "results_df_html = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(results_df_html))\n",
        "display(results_ui.get_container())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## 4. Clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRY_3wh1GVNm"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "delete_job = False\n",
        "delete_run = False\n",
        "delete_tutorial = False\n",
        "\n",
        "if delete_bucket:\n",
        "    ! gsutil rm -r {BUCKET_URI}\n",
        "\n",
        "if delete_job:\n",
        "    custom_job.delete()\n",
        "\n",
        "if delete_run:\n",
        "    ! gcloud functions delete 'custom_metric_case_2_3_transcription' --region={REGION}\n",
        "\n",
        "if delete_tutorial:\n",
        "    import shutil\n",
        "\n",
        "    shutil.rmtree(str(TUTORIAL_PATH))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2_3_vapo_transcription_custom_metric.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d83c3c05909493daa3e6342c3ae1790": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b055d264124452fbe66c768e807b010": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2efa849690394a7eba8b6830d906ba1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "gs://redstone_base_bucket/optimization_case2_3_custom/optimization_jobs/auto-prompt-design-job-gx25u6ll/instruction",
              "gs://redstone_base_bucket/optimization_case2_3_custom/optimization_jobs/auto-prompt-design-job-gx25u6ll/demonstration"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_ae946cfb0a194d6785150c21ad70f731",
            "style": "IPY_MODEL_c4b51a19bcbf48b2a99139d46dd77e9e"
          }
        },
        "327becb8d5004250bf81c55b8f4dfae8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "446a009355a849c2996fe9889314dc31": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4a37991304204a369f32b5bf0621452d",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<style type=\"text/css\">\n</style>\n<table id=\"T_8b156\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_8b156_level0_col0\" class=\"col_heading level0 col0\" >step</th>\n      <th id=\"T_8b156_level0_col1\" class=\"col_heading level0 col1\" >prompt</th>\n      <th id=\"T_8b156_level0_col2\" class=\"col_heading level0 col2\" >metrics.custom_engagement_personalization_score/mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_8b156_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_8b156_row0_col0\" class=\"data row0 col0\" ><div class=\"scrollable\">0</div></td>\n      <td id=\"T_8b156_row0_col1\" class=\"data row0 col1\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_8b156_row0_col2\" class=\"data row0 col2\" ><div class=\"scrollable\">2.7798262586820797</div></td>\n    </tr>\n  </tbody>\n</table>\n",
                  "text/plain": "<IPython.core.display.HTML object>"
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\n"
                ]
              },
              {
                "data": {
                  "text/html": "<style type=\"text/css\">\n</style>\n<table id=\"T_1d9a2\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_1d9a2_level0_col0\" class=\"col_heading level0 col0\" >target</th>\n      <th id=\"T_1d9a2_level0_col1\" class=\"col_heading level0 col1\" >audio</th>\n      <th id=\"T_1d9a2_level0_col2\" class=\"col_heading level0 col2\" >prompt</th>\n      <th id=\"T_1d9a2_level0_col3\" class=\"col_heading level0 col3\" >response</th>\n      <th id=\"T_1d9a2_level0_col4\" class=\"col_heading level0 col4\" >reference</th>\n      <th id=\"T_1d9a2_level0_col5\" class=\"col_heading level0 col5\" >custom_engagement_personalization_score/score</th>\n      <th id=\"T_1d9a2_level0_col6\" class=\"col_heading level0 col6\" >custom_engagement_personalization_score/explanation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_1d9a2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_1d9a2_row0_col0\" class=\"data row0 col0\" ><div class=\"scrollable\">대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다</div></td>\n      <td id=\"T_1d9a2_row0_col1\" class=\"data row0 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/20.wav</div></td>\n      <td id=\"T_1d9a2_row0_col2\" class=\"data row0 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row0_col3\" class=\"data row0 col3\" ><div class=\"scrollable\">대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다.</div></td>\n      <td id=\"T_1d9a2_row0_col4\" class=\"data row0 col4\" ><div class=\"scrollable\">대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다</div></td>\n      <td id=\"T_1d9a2_row0_col5\" class=\"data row0 col5\" ><div class=\"scrollable\">3.0</div></td>\n      <td id=\"T_1d9a2_row0_col6\" class=\"data row0 col6\" ><div class=\"scrollable\">Upon comparing the inference text against the reference text, there are **no errors** (no substitutions, deletions, or insertions) found in this example. The inference is an exact match to the reference.\n\n*   **Reference:** 대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다\n*   **Inference:** 대부분 학생들이 가장 비판적인 청중이므로 블로그 작가는 비판을 피하기 위해 글쓰기를 향상시키려고 노력하기 시작합니다\n\nThis indicates perfect performance for this specific utterance.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_1d9a2_row1_col0\" class=\"data row1 col0\" ><div class=\"scrollable\">이 도시는 전통적인 터키의 모습보다 지중해 유럽의 분위기를 더 많이 가지고 있지만 넓은 대로 정면이 유리로 장식된 건물 현대적인 쇼핑센터들 사이에 띄엄띄엄 전통적인 붉은 기와 지붕과 18세기 시장 그리고 오래된 모스크와 교회들이 있습니다</div></td>\n      <td id=\"T_1d9a2_row1_col1\" class=\"data row1 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/3.wav</div></td>\n      <td id=\"T_1d9a2_row1_col2\" class=\"data row1 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row1_col3\" class=\"data row1 col3\" ><div class=\"scrollable\">이 도시는 정통적인 터키의 모습보다 지중해 유럽의 분위기를 더 많이 가지고 있지만 넓은 대로 정면이 유리로 장식된 건물, 현대적인 쇼핑센터들 사이에 띄엄띄엄 정통적인 붉은 기와 지붕과 18세기 시장 그리고 오래된 모스크와 교회들이 있습니다.</div></td>\n      <td id=\"T_1d9a2_row1_col4\" class=\"data row1 col4\" ><div class=\"scrollable\">이 도시는 전통적인 터키의 모습보다 지중해 유럽의 분위기를 더 많이 가지고 있지만 넓은 대로 정면이 유리로 장식된 건물 현대적인 쇼핑센터들 사이에 띄엄띄엄 전통적인 붉은 기와 지붕과 18세기 시장 그리고 오래된 모스크와 교회들이 있습니다</div></td>\n      <td id=\"T_1d9a2_row1_col5\" class=\"data row1 col5\" ><div class=\"scrollable\">2.9090909091</div></td>\n      <td id=\"T_1d9a2_row1_col6\" class=\"data row1 col6\" ><div class=\"scrollable\">The STT performance for this item is very good, with only minor substitution errors.\n\nHere's a breakdown of the errors:\n\n1.  **Substitution:**\n    *   **Reference:** \"전통적인\" (jeon-tong-jeok-in - traditional)\n    *   **Inference:** \"정통적인\" (jeong-tong-jeok-in - authentic/orthodox)\n    *   This substitution occurs twice in the sentence. While phonetically similar, they have distinct meanings.\n\n**Summary of Errors:**\n\n*   **Substitutions:** 2 (\"정통적인\" instead of \"전통적인\" twice)\n*   **Deletions:** 0\n*   **Insertions:** 0\n\n**Performance Evaluation:**\nThe STT system correctly transcribed the vast majority of the words. The two substitutions are phonetically close, which indicates the system might have struggled with the subtle difference in initial consonant sound (ㅈ vs ㅊ) or chosen a less common but plausible word based on acoustic input.\n\n**Word Error Rate (WER):**\n*   Number of words in reference: 38\n*   Number of substitutions: 2\n*   Number of deletions: 0\n*   Number of insertions: 0\n*   WER = (S + D + I) / N = (2 + 0 + 0) / 38 = 2/38 ≈ 5.26%\n\nA WER of approximately 5.26% indicates a high level of accuracy for this specific utterance.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_1d9a2_row2_col0\" class=\"data row2 col0\" ><div class=\"scrollable\">이제 과학적 데이터가 거대한 규모의 이 탄소 경제는 지난 이백만 년 동안 인류의 진화를 도왔던 안정적인 상태에서 생물권을 몰아냈음을 보여줍니다</div></td>\n      <td id=\"T_1d9a2_row2_col1\" class=\"data row2 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/0.wav</div></td>\n      <td id=\"T_1d9a2_row2_col2\" class=\"data row2 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row2_col3\" class=\"data row2 col3\" ><div class=\"scrollable\">이제 과학적 데이터가 거대한 규모의 이 탄소 경제는 지난 200만 년 동안 인류의 진화를 도왔던 안정적인 상태에서 생물권을 보란했음을 보여줍니다.</div></td>\n      <td id=\"T_1d9a2_row2_col4\" class=\"data row2 col4\" ><div class=\"scrollable\">이제 과학적 데이터가 거대한 규모의 이 탄소 경제는 지난 이백만 년 동안 인류의 진화를 도왔던 안정적인 상태에서 생물권을 몰아냈음을 보여줍니다</div></td>\n      <td id=\"T_1d9a2_row2_col5\" class=\"data row2 col5\" ><div class=\"scrollable\">2.7481012658</div></td>\n      <td id=\"T_1d9a2_row2_col6\" class=\"data row2 col6\" ><div class=\"scrollable\">The STT performance for this item is fairly good, with only two substitution errors.\n\nHere's a detailed comparison:\n\n**Inference:** 이제 과학적 데이터가 거대한 규모의 이 탄소 경제는 지난 **200만** 년 동안 인류의 진화를 도왔던 안정적인 상태에서 생물권을 **보란했음을** 보여줍니다\n**Reference:** 이제 과학적 데이터가 거대한 규모의 이 탄소 경제는 지난 **이백만** 년 동안 인류의 진화를 도왔던 안정적인 상태에서 생물권을 **몰아냈음을** 보여줍니다\n\n**Errors Identified:**\n\n1.  **Substitution:**\n    *   **Reference:** `이백만` (written out number)\n    *   **Inference:** `200만` (numerical digit)\n    *   *Comment:* While the meaning is identical, the representation is different. This is a common STT variation.\n\n2.  **Substitution:**\n    *   **Reference:** `몰아냈음을` (meaning \"pushed out,\" \"expelled\")\n    *   **Inference:** `보란했음을` (This word \"보란\" doesn't form a common verb phrase in this context; it sounds like a misrecognition of \"몰아낸\" or a similar word, resulting in a semantically incorrect word.)\n    *   *Comment:* This is a significant error as it changes the meaning of the sentence.\n\n**Summary of Errors:**\n*   **Substitutions:** 2 (`200만` for `이백만`, `보란했음을` for `몰아냈음을`)\n*   **Deletions:** 0\n*   **Insertions:** 0</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_1d9a2_row3_col0\" class=\"data row3 col0\" ><div class=\"scrollable\">이것은 젊은 공룡의 꼬리이지만 샘플을 보면 어린 개체의 솜털이 아닌 성체의 깃털임을 알 수 있다고 연구진은 말합니다</div></td>\n      <td id=\"T_1d9a2_row3_col1\" class=\"data row3 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/23.wav</div></td>\n      <td id=\"T_1d9a2_row3_col2\" class=\"data row3 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row3_col3\" class=\"data row3 col3\" ><div class=\"scrollable\">이것은 젊은 공룡의 꼬리이지만 샘플을 보면 어린 개체의 솜털이 아닌 성체의 깃털임을 알 수 있다고 연구진은 말합니다.</div></td>\n      <td id=\"T_1d9a2_row3_col4\" class=\"data row3 col4\" ><div class=\"scrollable\">이것은 젊은 공룡의 꼬리이지만 샘플을 보면 어린 개체의 솜털이 아닌 성체의 깃털임을 알 수 있다고 연구진은 말합니다</div></td>\n      <td id=\"T_1d9a2_row3_col5\" class=\"data row3 col5\" ><div class=\"scrollable\">3.0</div></td>\n      <td id=\"T_1d9a2_row3_col6\" class=\"data row3 col6\" ><div class=\"scrollable\">Let's compare the inference text against the reference text.\n\n**Inference:** 이것은 젊은 공룡의 꼬리이지만 샘플을 보면 어린 개체의 솜털이 아닌 성체의 깃털임을 알 수 있다고 연구진은 말합니다\n**Reference:** 이것은 젊은 공룡의 꼬리이지만 샘플을 보면 어린 개체의 솜털이 아닌 성체의 깃털임을 알 수 있다고 연구진은 말합니다\n\nUpon comparison, the inference text is **identical** to the reference text.\n\n**Analysis of Errors:**\n*   **Substitutions:** 0\n*   **Deletions:** 0\n*   **Insertions:** 0\n\n**Conclusion:**\nThe STT performance for this item is perfect, with no errors detected.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_1d9a2_row4_col0\" class=\"data row4 col0\" ><div class=\"scrollable\">이것은 젊은 공룡의 꼬리이지만 샘플을 보면 어린 개체의 솜털이 아닌 성체의 깃털임을 알 수 있다고 연구진은 말합니다</div></td>\n      <td id=\"T_1d9a2_row4_col1\" class=\"data row4 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/8.wav</div></td>\n      <td id=\"T_1d9a2_row4_col2\" class=\"data row4 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row4_col3\" class=\"data row4 col3\" ><div class=\"scrollable\">이것은 젊은 공룡의 꼬리이지만 샘플을 보면 어린 개체의 솜털이 아닌 성체 깃털임을 알 수 있다고 연구진은 말합니다.</div></td>\n      <td id=\"T_1d9a2_row4_col4\" class=\"data row4 col4\" ><div class=\"scrollable\">이것은 젊은 공룡의 꼬리이지만 샘플을 보면 어린 개체의 솜털이 아닌 성체의 깃털임을 알 수 있다고 연구진은 말합니다</div></td>\n      <td id=\"T_1d9a2_row4_col5\" class=\"data row4 col5\" ><div class=\"scrollable\">2.9099264706</div></td>\n      <td id=\"T_1d9a2_row4_col6\" class=\"data row4 col6\" ><div class=\"scrollable\">Here's the evaluation of the STT performance:\n\n**Error Analysis:**\n\n*   **Deletion:**\n    *   **Reference:** \"성체**의** 깃털임을\"\n    *   **Inference:** \"성체 깃털임을\"\n    *   The word \"의\" (ui) is missing in the inference after \"성체\".</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_1d9a2_row5_col0\" class=\"data row5 col0\" ><div class=\"scrollable\">어둠 속에서 피라미드를 볼 수 있고 쇼가 시작하기 전에 고요 속에서 피라미드를 볼 수 있습니다</div></td>\n      <td id=\"T_1d9a2_row5_col1\" class=\"data row5 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/7.wav</div></td>\n      <td id=\"T_1d9a2_row5_col2\" class=\"data row5 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row5_col3\" class=\"data row5 col3\" ><div class=\"scrollable\">어둠 속에서 피라미드를 볼 수 있고 쇼가 시작하기 전에 고요 속에서 피라미드를 볼 수 있습니다.</div></td>\n      <td id=\"T_1d9a2_row5_col4\" class=\"data row5 col4\" ><div class=\"scrollable\">어둠 속에서 피라미드를 볼 수 있고 쇼가 시작하기 전에 고요 속에서 피라미드를 볼 수 있습니다</div></td>\n      <td id=\"T_1d9a2_row5_col5\" class=\"data row5 col5\" ><div class=\"scrollable\">3.0</div></td>\n      <td id=\"T_1d9a2_row5_col6\" class=\"data row5 col6\" ><div class=\"scrollable\">The STT performance is **excellent**.\n\nUpon comparing the inference text against the reference text, **no errors were found**. Both texts are identical.\n\n*   **Inference:** 어둠 속에서 피라미드를 볼 수 있고 쇼가 시작하기 전에 고요 속에서 피라미드를 볼 수 있습니다\n*   **Reference:** 어둠 속에서 피라미드를 볼 수 있고 쇼가 시작하기 전에 고요 속에서 피라미드를 볼 수 있습니다\n\n**Error Analysis:**\n*   **Substitutions:** 0\n*   **Deletions:** 0\n*   **Insertions:** 0\n\nThis indicates a perfect match for this specific segment.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_1d9a2_row6_col0\" class=\"data row6 col0\" ><div class=\"scrollable\">하루에 열여덟 개의 메달만 주어지기 때문에 많은 국가가 메달 단상에 오르지 못했습니다</div></td>\n      <td id=\"T_1d9a2_row6_col1\" class=\"data row6 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/24.wav</div></td>\n      <td id=\"T_1d9a2_row6_col2\" class=\"data row6 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row6_col3\" class=\"data row6 col3\" ><div class=\"scrollable\">하루에 18개의 메달만 주어지기 때문에 많은 국가가 메달 단상에 오르지 못했습니다.</div></td>\n      <td id=\"T_1d9a2_row6_col4\" class=\"data row6 col4\" ><div class=\"scrollable\">하루에 열여덟 개의 메달만 주어지기 때문에 많은 국가가 메달 단상에 오르지 못했습니다</div></td>\n      <td id=\"T_1d9a2_row6_col5\" class=\"data row6 col5\" ><div class=\"scrollable\">2.6631205674</div></td>\n      <td id=\"T_1d9a2_row6_col6\" class=\"data row6 col6\" ><div class=\"scrollable\">Here's the comparison and error identification:\n\n**Comparison:**\n\n*   **inference:** 하루에 **18개의** 메달만 주어지기 때문에 많은 국가가 메달 단상에 오르지 못했습니다\n*   **reference:** 하루에 **열여덟 개의** 메달만 주어지기 때문에 많은 국가가 메달 단상에 오르지 못했습니다\n\n**Error Identification:**\n\nThe only error identified is a **substitution**:\n\n*   **Substitution:** The inference produced \"18개의\" (digit form) instead of \"열여덟 개의\" (spelled-out form) which was in the reference. While semantically the same, lexically it's a different word sequence and thus counted as an error in STT evaluation.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_1d9a2_row7_col0\" class=\"data row7 col0\" ><div class=\"scrollable\">폴란드 남자 시각 장애인 스키 선수 마키 크레젤maciej krezel과 가이드 안나 오가진스카anna ogarzynska가 슈퍼대회전super g에서 13위로 경기를 마쳤다. 한국의 박종석은 남자 좌식스키 슈퍼대회전super g에서 24위를 차지했다</div></td>\n      <td id=\"T_1d9a2_row7_col1\" class=\"data row7 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/4.wav</div></td>\n      <td id=\"T_1d9a2_row7_col2\" class=\"data row7 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row7_col3\" class=\"data row7 col3\" ><div class=\"scrollable\">폴란드 남자 시각 장애인 스키 선수 마키 크레젤과 가이드 안나 오가진스카가 슈퍼 대회전에서 13위로 경기를 마쳤다. 한국의 박종석은 남자 좌식 스키 슈퍼 대회전에서 24위를 차지했다.</div></td>\n      <td id=\"T_1d9a2_row7_col4\" class=\"data row7 col4\" ><div class=\"scrollable\">폴란드 남자 시각 장애인 스키 선수 마키 크레젤maciej krezel과 가이드 안나 오가진스카anna ogarzynska가 슈퍼대회전super g에서 13위로 경기를 마쳤다. 한국의 박종석은 남자 좌식스키 슈퍼대회전super g에서 24위를 차지했다</div></td>\n      <td id=\"T_1d9a2_row7_col5\" class=\"data row7 col5\" ><div class=\"scrollable\">1.967902601</div></td>\n      <td id=\"T_1d9a2_row7_col6\" class=\"data row7 col6\" ><div class=\"scrollable\">The STT performance is very good, with only minor spacing/tokenization differences.\n\nHere's a comparison and breakdown of the errors:\n\n**Reference:** 폴란드 남자 시각 장애인 스키 선수 마키 크레젤maciej krezel과 가이드 안나 오가진스카anna ogarzynska가 슈퍼대회전super g에서 13위로 경기를 마쳤다 한국의 박종석은 남자 좌식스키 슈퍼대회전super g에서 24위를 차지했다\n\n**Inference:** 폴란드 남자 시각 장애인 스키 선수 마키 크레젤과 가이드 안나 오가진스카가 슈퍼 대회전에서 13위로 경기를 마쳤다 한국의 박종석은 남자 좌식 스키 슈퍼 대회전에서 24위를 차지했다\n\n---\n\n**Error Analysis:**\n\n1.  **Reference:** `슈퍼대회전` (one compound word)\n    **Inference:** `슈퍼 대회전` (two separate words)\n    *   **Type of error:** Spacing/Tokenization difference. The compound word in the reference is split into two words in the inference. This is a very minor orthographic difference that does not change the meaning.\n\n2.  **Reference:** `좌식스키` (one compound word)\n    **Inference:** `좌식 스키` (two separate words)\n    *   **Type of error:** Spacing/Tokenization difference. Similar to the above, a compound word is split.\n\n3.  **Reference:** `슈퍼대회전` (one compound word)\n    **Inference:** `슈퍼 대회전` (two separate words)\n    *   **Type of error:** Spacing/Tokenization difference. (Repetition of error type 1).\n\n**No other substitutions, deletions, or insertions were found.** The English names/terms (`maciej krezel`, `anna ogarzynska`, `super g`) in the reference are likely annotations and not part of the spoken Korean, so their absence in the inference is correct and not considered an error.\n\n**Summary:**\nThe STT system accurately transcribed all spoken words. The only discrepancies are minor spacing differences where compound words in the reference text were transcribed as separate words by the STT system. From a semantic perspective, the inference is perfectly accurate.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_1d9a2_row8_col0\" class=\"data row8 col0\" ><div class=\"scrollable\">일부 보도에 따르면 공식 사망자 수는 8명으로 집계됐으며 공식 보도에 따르면 최대 30명이 부상을 있었다고 전해진다 그러나 최종 사망자 수는 아직 알려지지 않았다</div></td>\n      <td id=\"T_1d9a2_row8_col1\" class=\"data row8 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/28.wav</div></td>\n      <td id=\"T_1d9a2_row8_col2\" class=\"data row8 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row8_col3\" class=\"data row8 col3\" ><div class=\"scrollable\">일부 보도에 따르면 공식 사망자 수는 여덟 명으로 집계됐으며 공식 보도에 따르면 최대 30명이 부상을 입었다고 전해진다. 그러나 최종 사망자 수는 아직 알려지지 않았다.</div></td>\n      <td id=\"T_1d9a2_row8_col4\" class=\"data row8 col4\" ><div class=\"scrollable\">일부 보도에 따르면 공식 사망자 수는 8명으로 집계됐으며 공식 보도에 따르면 최대 30명이 부상을 있었다고 전해진다 그러나 최종 사망자 수는 아직 알려지지 않았다</div></td>\n      <td id=\"T_1d9a2_row8_col5\" class=\"data row8 col5\" ><div class=\"scrollable\">2.7806763285000002</div></td>\n      <td id=\"T_1d9a2_row8_col6\" class=\"data row8 col6\" ><div class=\"scrollable\">Here's a comparison of the inference text against the reference text, highlighting the identified errors:\n\n**Reference Text:**\n`일부 보도에 따르면 공식 사망자 수는 8명으로 집계됐으며 공식 보도에 따르면 최대 30명이 부상을 있었다고 전해진다 그러나 최종 사망자 수는 아직 알려지지 않았다`\n\n**Inference Text:**\n`일부 보도에 따르면 공식 사망자 수는 여덟 명으로 집계됐으며 공식 보도에 따르면 최대 30명이 부상을 입었다고 전해진다 그러나 최종 사망자 수는 아직 알려지지 않았다`\n\n---\n\n**Error Analysis:**\n\n1.  **Substitution (S):**\n    *   **Reference:** `8명으로` (8 people)\n    *   **Inference:** `여덟 명으로` (eight people)\n    *   **Comment:** The numeral \"8\" was transcribed as the Korean word \"여덟\". While semantically identical, it's a change in form.\n\n2.  **Substitution (S):**\n    *   **Reference:** `부상을 있었다고` (there were injuries)\n    *   **Inference:** `부상을 입었다고` (sustained injuries)\n    *   **Comment:** The verb \"있었다고\" (was/were) was transcribed as \"입었다고\" (received/sustained). While contextually very similar in meaning, it is a different word choice.\n\n**Summary of Errors:**\n\n*   **Substitutions:** 2\n*   **Deletions:** 0\n*   **Insertions:** 0\n\nThe STT performance is good, with only two minor substitutions that don't significantly alter the overall meaning of the sentence. The main difference is the transcription of a numeral versus its spoken word equivalent, and a slight verb choice variation.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_1d9a2_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_1d9a2_row9_col0\" class=\"data row9 col0\" ><div class=\"scrollable\">화요일에 베어 쓰러뜨릴 예정이었으나 긴급 법원의 판결로 구조되었다</div></td>\n      <td id=\"T_1d9a2_row9_col1\" class=\"data row9 col1\" ><div class=\"scrollable\">gs://redstone_base_bucket/inputs/audio/17.wav</div></td>\n      <td id=\"T_1d9a2_row9_col2\" class=\"data row9 col2\" ><div class=\"scrollable\">\nGenerate a transcript of the speech. Only include the transcript in your response, and do not provide any other answer.\n</div></td>\n      <td id=\"T_1d9a2_row9_col3\" class=\"data row9 col3\" ><div class=\"scrollable\">화요일에 베어 쓰러뜨릴 예정이었으나 긴급 법원의 판결로 구제되었다.</div></td>\n      <td id=\"T_1d9a2_row9_col4\" class=\"data row9 col4\" ><div class=\"scrollable\">화요일에 베어 쓰러뜨릴 예정이었으나 긴급 법원의 판결로 구조되었다</div></td>\n      <td id=\"T_1d9a2_row9_col5\" class=\"data row9 col5\" ><div class=\"scrollable\">2.8194444444</div></td>\n      <td id=\"T_1d9a2_row9_col6\" class=\"data row9 col6\" ><div class=\"scrollable\">Here's an evaluation of the STT performance based on the provided items:\n\n**Overall Performance:** The STT system performed very well, accurately transcribing most of the sentence. There was only one error identified.\n\n**Detailed Error Analysis:**\n\n*   **Substitution:**\n    *   **Reference:** `구조되었다` (gu-jo-doe-eot-da - \"was rescued/saved from a physical danger\")\n    *   **Inference:** `구제되었다` (gu-je-doe-eot-da - \"was relieved/saved from a difficult situation,\" often legal or financial)\n    *   **Explanation:** The word `구조되었다` in the reference was substituted with `구제되었다` in the inference. These two words are very similar in pronunciation in Korean, differing by only one vowel sound, making it a common phonetic confusion for STT systems.</div></td>\n    </tr>\n  </tbody>\n</table>\n",
                  "text/plain": "<IPython.core.display.HTML object>"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "4a37991304204a369f32b5bf0621452d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "600px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "20px 0px 0px 0px",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": "auto",
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51de9c26d81646d39cb8083d584b04a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63b6be1599284ce6896d7328237de667": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac9194eab40a4a0d877078eca8028ee0",
            "placeholder": "​",
            "style": "IPY_MODEL_9848bc9f3df049e6921e94b18b207848",
            "value": "Select Template:"
          }
        },
        "7b6f9ef487b94a78bcced47df0f2fead": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e93f2ffdf2c7490ead17bf6923cac0ff",
              "IPY_MODEL_c91a5dd8906a48fab5115ee1da6694d4",
              "IPY_MODEL_c19a5dcd918d49369194452e0b071744"
            ],
            "layout": "IPY_MODEL_e8d512bf39a44c4b82cc0af8adb9e45a"
          }
        },
        "83457b2b62b242739b731c2ab69fea1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901bca8937324f3f800dd1158b110dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Template 0 custom_engagement_personalization_score: 2.7798262586820797",
              "Template 1 custom_engagement_personalization_score: 2.7897996614555423",
              "Template 2 custom_engagement_personalization_score: 2.7477310386315237",
              "Template 3 custom_engagement_personalization_score: 2.8135142019444914",
              "Template 4 custom_engagement_personalization_score: 2.7951885775226595",
              "Template 5 custom_engagement_personalization_score: 2.80778351423152",
              "Template 6 custom_engagement_personalization_score: 2.820048532592746",
              "Template 7 custom_engagement_personalization_score: 2.855458067616204",
              "Template 8 custom_engagement_personalization_score: 2.820048532592746",
              "Template 9 custom_engagement_personalization_score: 2.851832576516718",
              "Template 10 custom_engagement_personalization_score: 2.880146677476263",
              "Template 11 custom_engagement_personalization_score: 2.8376768508994927",
              "Template 12 custom_engagement_personalization_score: 2.841855682805646",
              "Template 13 custom_engagement_personalization_score: 2.801021774311896",
              "Template 14 custom_engagement_personalization_score: 2.8802598078686996",
              "Template 15 custom_engagement_personalization_score: 2.873648485509001",
              "Template 16 custom_engagement_personalization_score: 2.8825433091219956",
              "Template 17 custom_engagement_personalization_score: 2.8928547445775608",
              "Template 18 custom_engagement_personalization_score: 2.8861308928896796",
              "Template 19 custom_engagement_personalization_score: 2.9041864484452353",
              "Template 20 custom_engagement_personalization_score: 2.9030536149128445"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_c0bb0a7a7181424586b5439eff8a6428",
            "style": "IPY_MODEL_e7d9c10325fa4b8ca7762a281d31f46f"
          }
        },
        "9848bc9f3df049e6921e94b18b207848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac9194eab40a4a0d877078eca8028ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae946cfb0a194d6785150c21ad70f731": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "200px"
          }
        },
        "bb1594a3d51b46e892c69a75b10f3d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d575c6c9100446cc87a85c4b92566f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_0d83c3c05909493daa3e6342c3ae1790",
            "value": "Select Run:"
          }
        },
        "bca6cd712b584855b273017e48525824": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0bb0a7a7181424586b5439eff8a6428": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        },
        "c19a5dcd918d49369194452e0b071744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51de9c26d81646d39cb8083d584b04a4",
            "placeholder": "​",
            "style": "IPY_MODEL_f6dc4f9dc95f496bbf6a92008a43a897",
            "value": " 6.61k/? [00:00&lt;00:00, 550kB/s]"
          }
        },
        "c4b51a19bcbf48b2a99139d46dd77e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c91a5dd8906a48fab5115ee1da6694d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_327becb8d5004250bf81c55b8f4dfae8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b055d264124452fbe66c768e807b010",
            "value": 1
          }
        },
        "d575c6c9100446cc87a85c4b92566f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8eeefe199164a04b59576ea8a3fba21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7d9c10325fa4b8ca7762a281d31f46f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8d512bf39a44c4b82cc0af8adb9e45a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e93f2ffdf2c7490ead17bf6923cac0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83457b2b62b242739b731c2ab69fea1b",
            "placeholder": "​",
            "style": "IPY_MODEL_bca6cd712b584855b273017e48525824",
            "value": "Downloading builder script: "
          }
        },
        "edd3f054bb39449c8a981a6ab099b39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb1594a3d51b46e892c69a75b10f3d06",
              "IPY_MODEL_2efa849690394a7eba8b6830d906ba1d",
              "IPY_MODEL_63b6be1599284ce6896d7328237de667",
              "IPY_MODEL_901bca8937324f3f800dd1158b110dc4",
              "IPY_MODEL_446a009355a849c2996fe9889314dc31"
            ],
            "layout": "IPY_MODEL_d8eeefe199164a04b59576ea8a3fba21"
          }
        },
        "f6dc4f9dc95f496bbf6a92008a43a897": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
