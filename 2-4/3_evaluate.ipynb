{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.genai as genai\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "PROJECT_ID = '$YOUR_PROJECT_ID'\n",
    "my_api_key = os.getenv('GEMINI_API_KEY')\n",
    "client = genai.Client(vertexai=False, api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./sample_data/test-2-4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7687a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Using given image, please answer question below.\n",
    "Respond only with left or right.\n",
    "\n",
    "- Question : '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e9e4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "test_df['initial'] = None\n",
    "\n",
    "for idx in range(len(test_df)):\n",
    "    now_img_path = test_df.loc[idx, 'image_path']\n",
    "    now_image = Image.open(now_img_path)\n",
    "    now_question = test_df.loc[idx, 'question']\n",
    "\n",
    "    now_prompt = prompt + now_question\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=[now_prompt, now_image],\n",
    "        )\n",
    "    \n",
    "    test_df.loc[idx, 'initial'] = response.text\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d76fc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thoroughly examine the provided image to discern all relevant visual details.\n",
      "From the camera's objective viewpoint, precisely identify the left or right spatial orientation, position, or characteristic that directly answers the question in the query. All determinations of 'left' or 'right' must strictly adhere to the camera's perspective, not any subject's internal or anatomical left/right.\n",
      "When determining 'left' or 'right' for an object's facing direction, identify the horizontal side of the *image frame* (from the camera's viewpoint) towards which the object's primary face or front is directed. This is *not* the object's anatomical left or right, nor its internal gaze direction. If the query refers to multiple entities or an aggregate, determine the predominant horizontal facing direction as seen by the camera.\n",
      "When determining 'left' or 'right' for relative positions or features, always use the camera's objective viewpoint. First, identify the specific parts or entities mentioned. If the query asks about a characteristic that implies a vertical comparison (e.g., \"higher\" or \"lower\"), identify the entity or feature that satisfies the vertical condition, then determine its horizontal placement (left or right) *strictly from the camera's perspective*. For all other relative position queries, determine the horizontal placement of the specified parts or features *as they appear from the camera's viewpoint*.\n",
      "Confirm your interpretation based solely on the visual evidence.\n",
      "Respond only with left or right.\n",
      " - query: \n"
     ]
    }
   ],
   "source": [
    "optimized_2 = '''Thoroughly examine the provided image to discern all relevant visual details.\\nFrom the camera's objective viewpoint, precisely identify the left or right spatial orientation, position, or characteristic that directly answers the question in the query. All determinations of 'left' or 'right' must strictly adhere to the camera's perspective, not any subject's internal or anatomical left/right.\\nWhen determining 'left' or 'right' for an object's facing direction, identify the horizontal side of the *image frame* (from the camera's viewpoint) towards which the object's primary face or front is directed. This is *not* the object's anatomical left or right, nor its internal gaze direction. If the query refers to multiple entities or an aggregate, determine the predominant horizontal facing direction as seen by the camera.\\nWhen determining 'left' or 'right' for relative positions or features, always use the camera's objective viewpoint. First, identify the specific parts or entities mentioned. If the query asks about a characteristic that implies a vertical comparison (e.g., \\\"higher\\\" or \\\"lower\\\"), identify the entity or feature that satisfies the vertical condition, then determine its horizontal placement (left or right) *strictly from the camera's perspective*. For all other relative position queries, determine the horizontal placement of the specified parts or features *as they appear from the camera's viewpoint*.\\nConfirm your interpretation based solely on the visual evidence.\\nRespond only with left or right.\\n - query: '''\n",
    "print(optimized_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ceaff287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "test_df['optimized'] = None\n",
    "for idx in range(len(test_df)):\n",
    "    now_img_path = test_df.loc[idx, 'image_path']\n",
    "    now_image = Image.open(now_img_path)\n",
    "    now_question = test_df.loc[idx, 'question']\n",
    "\n",
    "    now_prompt = optimized_2 + now_question\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=[now_prompt, now_image],\n",
    "        )\n",
    "    \n",
    "    test_df.loc[idx, 'optimized'] = response.text\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19e8704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714285714285714\n",
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(test_df[test_df['answer'] == test_df['initial'].str.lower()].shape[0] / len(test_df))\n",
    "print(test_df[test_df['answer'] == test_df['optimized'].str.lower()].shape[0] / len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab351ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
