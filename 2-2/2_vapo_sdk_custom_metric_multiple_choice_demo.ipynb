{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g65MmV_HKi7I"
      },
      "source": [
        "## VAPO Custom metric Example 2-2\n",
        "\n",
        "- [Original Reference](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/prompt_optimizer/vertex_ai_prompt_optimizer_sdk_custom_metric.ipynb) from [Ivan Nardini](https://github.com/inardini)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HKyj5KwYePX"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This notebook demonstrates how to leverage Vertex AI prompt optimizer to optimize a simple prompt for a Gemini model using your own metric. The goal is to use Vertex AI prompt optimizer to find a new prompt template that generates better responses based on your own optimization metric.\n",
        "\n",
        "This tutorial uses the following Google Cloud services and resources:\n",
        "\n",
        "- Generative AI on Vertex AI\n",
        "- Vertex AI prompt optimizer\n",
        "- Vertex AI Gen AI evaluation\n",
        "- Vertex AI Custom job\n",
        "- Cloud Run\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "1. Define the prompt template you want to optimize.\n",
        "2. Prepare the prompt optimization dataset.\n",
        "3. Define and deploy your own custom evaluation metric on Cloud function.\n",
        "4. Set optimization mode and steps.\n",
        "5. Run the automatic prompt optimization job.\n",
        "6. Collect the best prompt template and eval metric.\n",
        "7. Validate the best prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## 2. Before you start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enFkr5SXo36F",
        "outputId": "cc7696be-7c3c-4cf8-bc7d-489c8f0023e1"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet 'google-cloud-aiplatform[evaluation]'\n",
        "%pip install --upgrade --quiet 'plotly' 'asyncio' 'tqdm' 'tenacity' 'etils' 'importlib_resources' 'fsspec' 'gcsfs' 'nbformat>=4.2.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "RrKXzTVMDKb4",
        "outputId": "d5c1fee3-739d-4a23-b5e2-2d40645ccb87"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py\n",
        "import vapo_lib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55e2195ce2d",
        "outputId": "319726c0-673b-4239-d90e-4f166200c764"
      },
      "outputs": [],
      "source": [
        "! mkdir -p ./tutorial/utils && wget https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/prompts/prompt_optimizer/vapo_lib.py -P ./tutorial/utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "Authenticate your environment on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyKGtVQjgx13",
        "outputId": "f00de733-2f20-4bbb-c67e-e94123ba5b2e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    try:\n",
        "        import google.auth\n",
        "        import google.auth.transport.requests\n",
        "        from google.colab import auth\n",
        "\n",
        "        auth.authenticate_user()\n",
        "        creds, project = google.auth.default()\n",
        "        authentication = google.auth.transport.requests.Request()\n",
        "        if creds.token:\n",
        "            print(\"Authentication successful.\")\n",
        "        else:\n",
        "            print(\"Authentication successful, but no token was returned.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Colab authentication: {e}\")\n",
        "\n",
        "! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the following APIs](https://console.cloud.google.com/flows/enableapi?apiid=cloudresourcemanager.googleapis.com,aiplatform.googleapis.com,cloudfunctions.googleapis.com,run.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID and project number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM1iC_MfAts1",
        "outputId": "539ac664-b963-4386-8a66-a43239f99b12"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"$YOUR_PROJECT_ID\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZpm-sL8f1z_"
      },
      "outputs": [],
      "source": [
        "PROJECT_NUMBER = !gcloud projects describe {PROJECT_ID} --format=\"get(projectNumber)\"[0]\n",
        "PROJECT_NUMBER = PROJECT_NUMBER[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6FmBV2_0fBP"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\" #\"asia-northeast3\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "#### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"$YOUR_BUCKET_NAME\"  # @param {type:\"string\"}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "# ! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "set_service_account"
      },
      "source": [
        "#### Service Account and permissions\n",
        "\n",
        "Vertex AI Prompt optimizer requires a service account with the following permissions:\n",
        "\n",
        "-   `Vertex AI User` to call Vertex LLM API\n",
        "-   `Storage Object Admin` to read and write to your GCS bucket.\n",
        "-   `Artifact Registry Reader` to download the pipeline template from Artifact Registry.\n",
        "-   `Cloud Run Developer` to deploy function on Cloud Run.\n",
        "\n",
        "[Check out the documentation](https://cloud.google.com/iam/docs/manage-access-service-accounts#iam-view-access-sa-gcloud) to learn how to grant those permissions to a single service account.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNRKI99_QtGR"
      },
      "source": [
        "> If you run following commands using Vertex AI Workbench, please directly run in the terminal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssUJJqXJJHgC"
      },
      "outputs": [],
      "source": [
        "SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqOHg5aid6HP",
        "outputId": "6efd0ddf-78ac-4ac5-ac42-03a4ac8cf958"
      },
      "outputs": [],
      "source": [
        "for role in ['aiplatform.user', 'storage.objectAdmin', 'artifactregistry.reader', 'run.developer', 'run.invoker']:\n",
        "\n",
        "    ! gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
        "      --member=serviceAccount:{SERVICE_ACCOUNT} \\\n",
        "      --role=roles/{role} --condition=None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek1-iTbPjzdJ"
      },
      "source": [
        "### Set tutorial folder and workspace\n",
        "\n",
        "Set a local folder to collect and organize data and any tutorial artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbfKRabXj3la"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path as path\n",
        "\n",
        "ROOT_PATH = path.cwd()\n",
        "TUTORIAL_PATH = ROOT_PATH / \"tutorial_case2_2\"\n",
        "BUILD_PATH = TUTORIAL_PATH / \"build_case2_2\"\n",
        "\n",
        "TUTORIAL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "BUILD_PATH.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaNdfftpXTIX"
      },
      "source": [
        "Set an associated workspace to store prompt optimization results on Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joJPc3FmX1fk"
      },
      "outputs": [],
      "source": [
        "from etils import epath\n",
        "\n",
        "WORKSPACE_URI = epath.Path(BUCKET_URI) / \"optimization_case2_2\"\n",
        "INPUT_DATA_URI = epath.Path(WORKSPACE_URI) / \"data_case2_2\"\n",
        "\n",
        "WORKSPACE_URI.mkdir(parents=True, exist_ok=True)\n",
        "INPUT_DATA_URI.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# Tutorial\n",
        "from argparse import Namespace\n",
        "import json\n",
        "\n",
        "# General\n",
        "import logging\n",
        "from pprint import pprint\n",
        "import warnings\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from google.cloud import aiplatform\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tutorial.utils import vapo_lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "820DIvw1o8tB"
      },
      "source": [
        "### Libraries settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKc4ZdUBo_SM"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxc7q4r-DFH4"
      },
      "source": [
        "### Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y5t67f3DHNm"
      },
      "outputs": [],
      "source": [
        "INPUT_DATA_FILE_URI = \"$YOUR_FILE_PATH\"\n",
        "\n",
        "INPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / \"prompt_optimization_data\"\n",
        "INPUT_OPTIMIZATION_DATA_FILE_URI = str(\n",
        "    INPUT_DATA_URI / \"prompt_optimization_dataset.jsonl\"\n",
        ")\n",
        "OUTPUT_OPTIMIZATION_DATA_URI = epath.Path(WORKSPACE_URI) / \"optimization_jobs\"\n",
        "APD_CONTAINER_URI = (\n",
        "    \"us-docker.pkg.dev/vertex-ai-restricted/builtin-algorithm/apd:preview_v1_0\"\n",
        ")\n",
        "CONFIG_FILE_URI = str(WORKSPACE_URI / \"config\" / \"config.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr4Kg-x-JplC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXUXmoMVVOmX",
        "outputId": "4adab76f-6b2c-43ca-9a42-87396e96e048"
      },
      "outputs": [],
      "source": [
        "print(REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQMc2Uwf0fBQ"
      },
      "outputs": [],
      "source": [
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "## 3. Automated prompt design with Vertex AI prompt optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmTotjRAJplw"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Load the dataset from a Google Cloud Storage bucket. The dataset contains the following columns:\n",
        "\n",
        "*   `question`: The multiple-choice question posed by the user.\n",
        "*   `choices`: Possible answer choices for the question\n",
        "*   `target`: The ground truth answer—the ideal response the user expects from the AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA7aG08wJtVm"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_df = pd.read_json(INPUT_DATA_FILE_URI, lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1xn-pz3v5HVK",
        "outputId": "f2432926-a628-48ce-cd03-8010506fbbfc"
      },
      "outputs": [],
      "source": [
        "prompt_optimization_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "b8zgOTZVa6xa",
        "outputId": "416f1c2c-0280-4bb8-db53-bd9ec4da3b8d"
      },
      "outputs": [],
      "source": [
        "vapo_lib.print_df_rows(prompt_optimization_df[['question', 'choices', 'target']], n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp1n1aMACzSW"
      },
      "source": [
        "### Optimize the prompt template with Vertex AI prompt optimizer with custom metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1650lf3X8xW"
      },
      "source": [
        "#### Prepare the prompt template you want to optimize\n",
        "\n",
        "A prompt consists of two key parts:\n",
        "\n",
        "* **System Instruction Template** which is a fixed part of the prompt that control or alter the model's behavior across all queries for a given task.\n",
        "\n",
        "* **Prompt Template** which is a dynamic part of the prompt that changes based on the task. Prompt template includes context, task and more. To learn more, see [components of a prompt](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies#components-of-a-prompt) in the official documentation.\n",
        "\n",
        "In this scenario, you use Vertex AI prompt optimizer to optimize a simple system instruction template. And you use some examples in the remaining prompt template for evaluating different instruction templates along the optimization process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edf107f3cc99"
      },
      "source": [
        "> Having the `target` placeholder in the prompt template is optional. It represents the prompt's ground truth response in your prompt optimization dataset that you aim to optimize for your templates. If you don't have the prompt's ground truth response, remember to set the `source_model` parameter to your prompt optimizer configuration (see below) instead of adding ground truth responses. Vertex AI prompt optimizer would run your sample prompts on the source model to generate the ground truth responses for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db8rHNC6DmtY"
      },
      "outputs": [],
      "source": [
        "SYSTEM_INSTRUCTION_TEMPLATE = \"\"\"\n",
        "You are a smart multiple-choice answering assistant.\n",
        "You're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n",
        "\n",
        "Please return a JSON object with three properties: \\\"answer\\\", \\\"reason\\\" and \\\"likelihood\\\".\n",
        "The \\\"answer\\\" property is a integer, indicating the right choice number.\n",
        "The \\\"reason\\\" property is a string that explains which of the comparisons are the best matches\n",
        "for the input, and the reason should also explain any factors that influence the likelihood score.\n",
        "The \\\"likelihood\\\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Some examples of correct \"question\" & \"choices\" & \"target\" are:\n",
        "question: {question}\n",
        "choices : {choices}\n",
        "target : {target}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1TCgXsrXztm"
      },
      "source": [
        "#### Prepare the prompt optimization dataset\n",
        "\n",
        "To use Vertex AI prompt optimizer, you'll need a CSV or JSONL file with labeled examples.  These examples should follow a specific naming convention. For details see [Optimize prompts](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYV-fansHMM_"
      },
      "source": [
        "> For effective **prompt optimization**, provide a dataset of examples where your model is poor in performance when using current system instruction template. For reliable results, use 50-100 distinct samples.\n",
        "\n",
        "> In case of **prompt migration**, consider using the source model to label examples that the target model struggles with, helping to identify areas for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBWLrWKil6jK"
      },
      "outputs": [],
      "source": [
        "train_prompt_optimization_df = prompt_optimization_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTIl_v9Ig1F-"
      },
      "outputs": [],
      "source": [
        "# Remove uneccessary columns\n",
        "prepared_train_prompt_optimization_df = train_prompt_optimization_df.drop(\n",
        "    columns=['answer', 'likelihood', 'reason']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbTqB2o5ZslT"
      },
      "source": [
        "Print some examples of the prompt optimization dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_DUFEAb82eEi",
        "outputId": "323cfcb9-dad5-4ce4-bb3a-99f4c87940a5"
      },
      "outputs": [],
      "source": [
        "prepared_train_prompt_optimization_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF3XY_d_yB-K"
      },
      "source": [
        "#### Upload samples to bucket\n",
        "\n",
        "Once you prepare your prompt optimization dataset, you can upload them on Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcPzAM6kMqUD",
        "outputId": "2cc3ff68-89c8-497b-ddea-ddf20fd824a2"
      },
      "outputs": [],
      "source": [
        "print(INPUT_OPTIMIZATION_DATA_FILE_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "155paLgGUXOm"
      },
      "outputs": [],
      "source": [
        "prepared_train_prompt_optimization_df.to_json(\n",
        "    INPUT_OPTIMIZATION_DATA_FILE_URI, orient=\"records\", lines=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxpid3KgAkYM"
      },
      "source": [
        "#### Define and deploy your own custom optimization metric on Cloud function\n",
        "\n",
        "To optimize your prompt template using a custom optimization metric, you need to deploy a function with your own metric code on a Cloud function. To deploy a Cloud function with your own custom metric, you cover the following steps:\n",
        "\n",
        "1.   Define requirements\n",
        "2.   Write your own custom metric function code\n",
        "3.   Deploy the custom code as Cloud function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxh2e88fAnQc"
      },
      "source": [
        "##### Define requirements\n",
        "\n",
        "Set the custom metric dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-hUlhgBCus4"
      },
      "outputs": [],
      "source": [
        "requirements = \"\"\"\n",
        "functions-framework==3.*\n",
        "google-cloud-aiplatform\n",
        "rouge_score\n",
        "scikit-learn\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(BUILD_PATH / \"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_EFZEBeEy48"
      },
      "source": [
        "##### Write your own custom metric function\n",
        "\n",
        "Define the module which contains your own custom metric function definition.\n",
        "\n",
        "In this case, you have a custom evaluation metric to evaluate the user engagement and personalization. The custom evaluation metric is defined using the `evaluate_engagement_personalization_fn`.\n",
        "\n",
        "The function leverages \"gemini-2.0-flash\" to act as an \"auto-rater\". It sends a prompt to the auto-rater, receives a score (1-5), and an explanation, then returns these as a dictionary containing two fields: the custom metric's score (as you defined it) and an explanation of how this metric helps optimize the prompt template.\n",
        "\n",
        "You use the `main` function to deploy the `evaluate_engagement_personalization_fn` function as a Cloud Function, receiving a question, response, and a target response as input and returning the auto-rater's evaluation.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wGoVQNCMbxe"
      },
      "outputs": [],
      "source": [
        "custom_metric_function_code = '''\n",
        "\"\"\"\n",
        "This module contains the custom evaluation metric definition to optimize a prompt template with Vertex AI prompt optimizer\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict\n",
        "from vertexai.generative_models import (\n",
        "  GenerationConfig,\n",
        "  GenerativeModel\n",
        ")\n",
        "\n",
        "import json\n",
        "import functions_framework\n",
        "from rouge_score import rouge_scorer\n",
        "import ast\n",
        "\n",
        "def get_autorater_response(metric_prompt: str) -> dict:\n",
        "  \"\"\"This function is to generate the evaluation response from the autorater.\"\"\"\n",
        "\n",
        "  metric_response_schema = {\n",
        "      \"type\": \"OBJECT\",\n",
        "      \"properties\": {\"eval_score\": {\"type\": \"NUMBER\"}, \"explanation\": {\"type\": \"STRING\"}},\n",
        "      \"required\": [\"eval_score\", \"explanation\"],\n",
        "  }\n",
        "\n",
        "  autorater = GenerativeModel(\n",
        "      \"gemini-2.5-flash\",\n",
        "      generation_config=GenerationConfig(\n",
        "          response_mime_type=\"application/json\",\n",
        "          response_schema=metric_response_schema,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = autorater.generate_content(metric_prompt)\n",
        "\n",
        "  response_json = {}\n",
        "  response_json = json.loads(response.text)\n",
        "  return response_json\n",
        "\n",
        "# Define custom evaluation criteria\n",
        "def evaluate_engagement_personalization_fn(r_answer:str, r_reason:str, r_likelihood:str,\n",
        "answer:str, reason:str, likelihood:str) -> Dict[str, str]:\n",
        "  custom_metric_prompt_template = \"\"\"\n",
        "  You are an expert AI evaluator. Your task is to assess the quality of a model's generated answer against a ground truth reference.\n",
        "  You must calculate a single, **holistic quality score** based on three weighted criteria: matching accuracy, likelihood accuracy, and reasoning quality.\n",
        "\n",
        "  -----\n",
        "  # **Input Format**\n",
        "\n",
        "  You will be provided with the model's prediction and the ground truth as separate string fields, not as a single JSON object.\n",
        "\n",
        "    * **Prediction Inputs:**\n",
        "        * `r_answer`: The predicted answer, provided as a string (e.g., `'1'`).\n",
        "        * `r_likelihood`: The predicted score, provided as a string (e.g., `'0.85'`).\n",
        "        * `r_reason`: The predicted textual explanation.\n",
        "    * **Ground Truth Inputs:**\n",
        "        * `answer`: The predicted answer, provided as a string (e.g., `'1'`).\n",
        "        * `likelihood`: The ground truth score, provided as a string.\n",
        "        * `reason`: The ground truth textual explanation.\n",
        "\n",
        "  -----\n",
        "\n",
        "  # **Evaluation Steps & Scoring Rubric**\n",
        "\n",
        "  You must calculate the final `\"eval_score\"` by following these steps precisely.\n",
        "  You must also provide step-by-step explanation for your `\"eval_score\"`.\n",
        "\n",
        "\n",
        "  ## **⚠️ Step 0: Parse Input Strings**\n",
        "\n",
        "  Before calculating scores, you must convert the input strings into the correct data types.\n",
        "\n",
        "    * Parse `r_answer` and `answer` from string representations into **integers**.\n",
        "    * Parse `r_likelihood` and `likelihood` from string representations into **floating-point numbers**.\n",
        "    * The `r_reason` and `reason` fields are already strings and require no parsing.\n",
        "\n",
        "  ## **1. Calculate Matching Accuracy (Weight: 50%)**\n",
        "\n",
        "  Evaluate the accuracy of the matched items. This score is $Score\\_M$.\n",
        "\n",
        "    * **Compare**: The parsed `r_answer` integer against the parsed `answer` integer.\n",
        "    * **Method**:\n",
        "        * If `r_answer` == `answer`, $Score\\_M$ = 1.0$.\n",
        "        * If `r_answer` != `answer`, $Score\\_M$ = 0.0$.\n",
        "\n",
        "  ## **2. Calculate Likelihood Accuracy (Weight: 30%)**\n",
        "\n",
        "  Evaluate the proximity of the predicted score to the ground truth score. This score is $Score\\_S$.\n",
        "\n",
        "    * **Compare**: The parsed `r_likelihood` number against the parsed `likelihood` number.\n",
        "    * **Method**: Calculate the accuracy based on the absolute difference.\n",
        "        * **Scoring Accuracy ($Score\\_S$)** = $1.0 - |\\\\text{predicted\\_score} - \\\\text{truth\\_score}|$\n",
        "\n",
        "  ## **3. Calculate Reasoning Quality (Weight: 20%)**\n",
        "\n",
        "  Assess the textual similarity of the reason. This score is $Score\\_R$.\n",
        "\n",
        "    * **Compare**: The `r_reason` string against the `reason` string.\n",
        "    * **Method**: Use the **ROUGE-L F-measure**, which measures the longest common subsequence of words.\n",
        "        * **Reasoning Quality ($Score\\_R$)** = ROUGE-L F-measure score.\n",
        "\n",
        "  ## **4. Generate Explanation **\n",
        "\n",
        "  Justify the assigned `\"eval_score\"` with a clear and concise explanation,\n",
        "  highlighting the strengths and weaknesses of the response with respect to each criterion.\n",
        "\n",
        "  -----\n",
        "\n",
        "  ## **5. Final Score Calculation**\n",
        "\n",
        "  Combine the three component scores using the specified weights to produce the final holistic score.\n",
        "\n",
        "  $$\\text{Final Score} = (0.5 \\times Score_M) + (0.3 \\times Score_S) + (0.2 \\times Score_R)$$\n",
        "\n",
        "  -----\n",
        "\n",
        "  # **Crucial Constraint: Invalid Input Format**\n",
        "\n",
        "  **This is a critical rule.** The evaluation depends on correctly parsing the model's prediction inputs.\n",
        "\n",
        "    * If the prediction string `r_answer` **cannot be parsed** into a integer,\n",
        "    or if `r_likelihood` **cannot be parsed** into a number, the evaluation is a complete failure.\n",
        "    * In such cases, you must **ignore all other calculations and output a final score of 0.0**.\n",
        "\n",
        "  # **Required Output Format**\n",
        "\n",
        "  Your final output must be a single JSON object containing two keys,\n",
        "  `\"eval_score\"`, with the calculated holistic score as a float and `\"explanation\"` with the explanation as a string\n",
        "\n",
        "  **Example:**\n",
        "\n",
        "  ```json\n",
        "  {\n",
        "    \"eval_score\": 0.85,\n",
        "    \"explanation\": \"Correct matches, but not accurate score\"\n",
        "  }\n",
        "  ```\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  input_template = \"\"\"\n",
        "\n",
        "  # **Inputs**\n",
        "\n",
        "  - `r_answer` : {r_answer}\n",
        "  - `r_reason`: {r_reason}\n",
        "  - `r_likelihood`: {r_likelihood}\n",
        "  - `answer` : {answer}\n",
        "  - `reason`: {reason}\n",
        "  - `likelihood`: {likelihood}\n",
        "  \"\"\"\n",
        "  real_input = input_template.format(r_answer=r_answer, r_reason=r_reason, r_likelihood=r_likelihood,\n",
        "  answer=answer, reason=reason, likelihood=likelihood)\n",
        "  custom_metric_prompt = custom_metric_prompt_template + real_input\n",
        "\n",
        "  response_dict = get_autorater_response(custom_metric_prompt)\n",
        "  print(f\"response : {response_dict}\")\n",
        "\n",
        "  return {\n",
        "      \"custom_engagement_personalization_score\": response_dict[\"eval_score\"],\n",
        "      \"explanation\": response_dict[\"explanation\"],\n",
        "  }\n",
        "\n",
        "def refine_string(text_data):\n",
        "    cleaned_text = text_data.strip()\n",
        "    if cleaned_text.startswith(\"```json\"):\n",
        "        cleaned_text = cleaned_text[len(\"```json\"):].strip()\n",
        "\n",
        "    if cleaned_text.endswith(\"```\"):\n",
        "        cleaned_text = cleaned_text[:-len(\"```\")].strip()\n",
        "\n",
        "    if cleaned_text.startswith('{') and cleaned_text.endswith('}'):\n",
        "        json_string = cleaned_text\n",
        "    else:\n",
        "        json_string = None\n",
        "\n",
        "    data_dict = {}\n",
        "    reason = None\n",
        "    if json_string:\n",
        "        try:\n",
        "            data_dict = json.loads(json_string)\n",
        "        except json.JSONDecodeError as e:\n",
        "          reason = 'Wrong JSON format'\n",
        "    else:\n",
        "      reason = 'Wrong JSON format'\n",
        "    return data_dict, reason\n",
        "\n",
        "\n",
        "# Register an HTTP function with the Functions Framework\n",
        "@functions_framework.http\n",
        "def main(request):\n",
        "  default_return = {\"custom_engagement_personalization_score\": 0, \"explanation\": \"Invalid JSON format.\"}\n",
        "  request_json = request.get_json(silent=True)\n",
        "  if not request_json:\n",
        "    raise ValueError('Cannot find request json.')\n",
        "\n",
        "  response = request_json['response']\n",
        "\n",
        "  try:\n",
        "    target_dict, parsing_reason = refine_string(request_json['target'])\n",
        "    if parsing_reason is not None:\n",
        "        print(\"Wrong JSON format : Target\")\n",
        "        return json.dumps(default_return)\n",
        "    else:\n",
        "      answer = target_dict['answer']\n",
        "      reason = target_dict['reason']\n",
        "      likelihood = target_dict['likelihood']\n",
        "\n",
        "    inference_dict, inference_parsing_reason = refine_string(response)\n",
        "    if inference_parsing_reason is not None:\n",
        "        print(\"Wrong JSON format : Inference\")\n",
        "        return json.dumps(default_return)\n",
        "    else:\n",
        "      r_answer = inference_dict['answer']\n",
        "      r_reason = inference_dict['reason']\n",
        "      r_likelihood = inference_dict['likelihood']\n",
        "\n",
        "    get_evaluation_result = evaluate_engagement_personalization_fn(r_answer,r_reason,r_likelihood,answer,reason,likelihood)\n",
        "    return json.dumps(get_evaluation_result)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"##Exception##\")\n",
        "    print(e)\n",
        "    print(\"##Response##\")\n",
        "    print(response)\n",
        "\n",
        "    return json.dumps(default_return)\n",
        "\n",
        "'''\n",
        "\n",
        "with open(BUILD_PATH / \"main.py\", \"w\") as f:\n",
        "    f.write(custom_metric_function_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7R0LDZMCPnL"
      },
      "source": [
        "##### Deploy the custom metric as a Cloud Function\n",
        "\n",
        "Use gcloud command line to deploy a Cloud function. To learn more, check out [Deploy a Python service to Cloud Run](https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-python-service) quickstart.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwBZGvkLCizs",
        "outputId": "8b8f07b5-04c0-4366-fc21-40d08e77a09c"
      },
      "outputs": [],
      "source": [
        "!gcloud functions deploy 'custom_metric_case_2_2' \\\n",
        " --gen2 \\\n",
        " --runtime=\"python310\" \\\n",
        " --source={str(BUILD_PATH)} \\\n",
        " --entry-point=main \\\n",
        " --trigger-http \\\n",
        " --timeout=3600 \\\n",
        " --memory=2Gb \\\n",
        " --concurrency=6 \\\n",
        " --min-instances=6 \\\n",
        " --project {PROJECT_ID} \\\n",
        " --region={REGION} \\\n",
        " --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoEypczSwAGK"
      },
      "source": [
        "##### Test your custom evaluation metric\n",
        "\n",
        "After you deploy your  custom evaluation metric as Cloud function, submit a request to validate the output of the custom evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA_6GO8BOK_h",
        "outputId": "4408999c-7e6c-40b3-9c27-16c425130922"
      },
      "outputs": [],
      "source": [
        "! gcloud functions describe 'custom_metric_case_2_2' --gen2 --region {REGION} --format=\"value(url)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXOWYp2MwEsA"
      },
      "outputs": [],
      "source": [
        "custom_evaluator_function_uri = ! gcloud functions describe 'custom_metric_case_2_2' --gen2 --region {REGION} --format=\"value(url)\"\n",
        "custom_evaluator_function_uri = custom_evaluator_function_uri[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbXhK3qnL1FW",
        "outputId": "53032ba2-ec58-4cb5-928c-a0cbd8517fc3"
      },
      "outputs": [],
      "source": [
        "print(custom_evaluator_function_uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gJRLNDW2_MJ4",
        "outputId": "c826c040-5d80-4ea1-f2dd-d340e5fde467"
      },
      "outputs": [],
      "source": [
        "prepared_train_prompt_optimization_df['target'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__TtIGTp_rBv"
      },
      "outputs": [],
      "source": [
        "response_ex = '```json\\n{\\n  \"answer\":2,\\n  \"reason\":\"\\\\\"RNA-associated Degradation\\\\\" is a specific biological process involving the breakdown of RNA molecules, and \\'RAID\\' is an acronym used in certain molecular biology contexts to refer to it. This usage is much less common compared to the acronym\\'s well-known meaning in computer storage or other fields.\",\\n  \"likelihood\":0.07\\n}\\n```'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9tzttxaB5fV",
        "outputId": "6cd48676-ba65-40aa-d37a-de2b6fea7904"
      },
      "outputs": [],
      "source": [
        "# def refine_string(text_data):\n",
        "#     cleaned_text = text_data.strip()\n",
        "#     if cleaned_text.startswith(\"```json\"):\n",
        "#         cleaned_text = cleaned_text[len(\"```json\"):].strip()\n",
        "\n",
        "#     if cleaned_text.endswith(\"```\"):\n",
        "#         cleaned_text = cleaned_text[:-len(\"```\")].strip()\n",
        "\n",
        "#     if cleaned_text.startswith('{') and cleaned_text.endswith('}'):\n",
        "#         json_string = cleaned_text\n",
        "#     else:\n",
        "#         json_string = None\n",
        "\n",
        "#     data_dict = {}\n",
        "#     reason = None\n",
        "#     if json_string:\n",
        "#         try:\n",
        "#             data_dict = json.loads(json_string)\n",
        "#         except json.JSONDecodeError as e:\n",
        "#           reason = 'Wrong JSON format'\n",
        "#     else:\n",
        "#       reason = 'Wrong JSON format'\n",
        "#     return data_dict, reason\n",
        "\n",
        "# refine_string(response_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JMeIyx0DHnc",
        "outputId": "3be12e12-4127-430e-d930-b2b2f5e40ef7"
      },
      "outputs": [],
      "source": [
        "dummy_token = 'ABC' # get_auth_token()\n",
        "#     \"Authorization\": f\"Bearer {dummy_token}\",\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "\n",
        "json_data = {\n",
        "    \"response\": response_ex,\n",
        "    \"target\": f\"\"\"{prepared_train_prompt_optimization_df['target'][0]}\"\"\",\n",
        "}\n",
        "\n",
        "response = requests.post(custom_evaluator_function_uri, headers=headers, json=json_data, timeout=70) #.json()\n",
        "pprint(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKo7t7BIvWUT",
        "outputId": "67948e88-509e-44fa-97a1-2df8d4c4223b"
      },
      "outputs": [],
      "source": [
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5RD0l2xX-FI"
      },
      "source": [
        "#### Configure optimization settings\n",
        "\n",
        "Vertex AI prompt optimizer lets you control the optimization process by specifying what to optimize (instructions only, demonstrations only, or both), providing a system instruction and prompt template, and selecting the target model.  You can optionally refine the optimization with some advanced settings like its duration and the number of optimization iterations it runs, which models the Vertex AI prompt optimizer uses, and other parameters to control the structure and content of prompts. Below you have some common and recommended default configurations.\n",
        "\n",
        "In this scenario, you set two additional parameters:\n",
        "\n",
        "* `custom_metric_name` parameter which lets you pass your own custom metric to optimizer the prompt template.\n",
        "\n",
        "* `custom_metric_cloud_function_name` parameter which indicates the Cloud function to call for collecting custom function evaluation metric output.\n",
        "\n",
        "For more advanced control, you can learn and explore more about all the parameters and how to best use them in the [detailed documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFHutXhgeqRx"
      },
      "outputs": [],
      "source": [
        "## Important setting : response_mime_type=\"application/json\"\n",
        "\n",
        "PROMPT_OPTIMIZATION_JOB = \"auto-prompt-design-job-\" + vapo_lib.get_id()\n",
        "OUTPUT_OPTIMIZATION_RUN_URI = str(\n",
        "    OUTPUT_OPTIMIZATION_DATA_URI / PROMPT_OPTIMIZATION_JOB\n",
        ")\n",
        "\n",
        "args = Namespace(\n",
        "    # Basic configuration\n",
        "    system_instruction=SYSTEM_INSTRUCTION_TEMPLATE,  # System instructions for the target model. String.\n",
        "    prompt_template=PROMPT_TEMPLATE,  # Template for prompts,  String.\n",
        "    target_model=\"gemini-2.5-flash\",  # Target model for optimization. String. Supported models: \"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-2.0-flash-lite-001\", \"gemini-2.0-flash-001\"\n",
        "    thinking_budget=-1,  # Thinking budget for thinking models. -1 means no thinking for non-thinking models and auto thinking for thinking models. Integer.\n",
        "    optimization_mode=\"instruction_and_demo\",  # Optimization mode. String. Supported modes: \"instruction\", \"demonstration\", \"instruction_and_demo\"\n",
        "    custom_metric_name=\"custom_engagement_personalization_score\",  # Metric name, as defined by the key that corresponds in the dictionary returned from Cloud function. String.\n",
        "    custom_metric_cloud_function_name=\"custom_metric_case_2_2\",  # Cloud Run function name you previously deployed. String.\n",
        "    eval_metrics_types=[\n",
        "        \"custom_metric\",\n",
        "    ],  # List of evaluation metrics. List of strings. Supported metrics: \"bleu\", \"coherence\", \"comet\", \"exact_match\", \"fluency\", \"groundedness\", \"metricx\", \"rouge_1\", \"rouge_2\", \"rouge_l\", \"rouge_l_sum\", \"safety\", \"question_answering_correctness\", \"question_answering_quality\", \"summarization_quality\", \"text_quality\", \"verbosity\", \"tool_call_valid\", \"tool_name_match\", \"tool_parameter_key_match\", \"tool_parameter_kv_match\"\n",
        "    eval_metrics_weights=[\n",
        "        1.0\n",
        "    ],  # Weights for evaluation metrics. List of floats.  Length must match eval_metrics_types.  Should sum to 1.\n",
        "    aggregation_type=\"weighted_sum\",  # Aggregation type for evaluation metrics. String. Supported aggregation types: \"weighted_sum\", \"weighted_average\"\n",
        "    input_data_path=INPUT_OPTIMIZATION_DATA_FILE_URI,  # Cloud Storage URI to input optimization data. String.\n",
        "    output_path=OUTPUT_OPTIMIZATION_RUN_URI,  # Cloud Storage URI to save optimization results. String.\n",
        "    project=PROJECT_ID,  # Google Cloud project ID. String.\n",
        "    # (Optional) Advanced configuration\n",
        "    num_steps=10,  # Number of iterations in instruction optimization mode. Integer between 10 and 20.\n",
        "    num_demo_set_candidates=10,  # Number of demonstrations evaluated in instruction and instruction_and_demo mode. Integer between 10 and 30.\n",
        "    demo_set_size=3,  # Number of demonstrations generated per prompt. Integer between 3 and 6.\n",
        "    target_model_location=\"us-central1\", # Location of the target model. String. Default us-central1.\n",
        "    optimizer_model_location=\"us-central1\", # Location of the optimizer model. String. Default us-central1.\n",
        "    source_model=\"\",  # Google model that the system instructions and prompts were previously used with. String. Not needed if you provide target column.\n",
        "    source_model_location=\"\",  # Location of the source model. String. Default us-central1. Not needed if you provide target column.\n",
        "    target_model_qps=1,  # The queries per second (QPS) sent to the target model. Integer greater or equal than 1 depending on your quota.\n",
        "    optimizer_model_qps=1,  # The queries per second (QPS) sent to the optimization model. Integer greater or equal than 1 depending on your quota.\n",
        "    eval_qps=1,  # The queries per second (QPS) sent to the eval model. Integer greater or equal than 1 depending on your quota.\n",
        "    source_model_qps=\"\",  # The queries per second (QPS) sent to the source model. Integer greater or equal than 1 depending on your quota.\n",
        "    response_mime_type=\"application/json\",  # MIME response type that the target model uses. String. Supported response: text/plain, text/x.enum, application/json.\n",
        "    response_schema=\"\",  # The Vertex AI's Controlled Generation response schema that the target model uses to generate answers. String.\n",
        "    language=\"English\",  # Language of the system instructions. String. Supported languages: \"English\", \"French\", \"German\", \"Hebrew\", \"Hindi\", \"Italian\", \"Japanese\", \"Korean\", \"Portuguese\", \"Simplified Chinese\", \"Spanish\", \"Traditional Chinese\"\n",
        "    placeholder_to_content=json.loads(\n",
        "        \"{}\"\n",
        "    ),  # Placeholder to replace any parameter in the system instruction. Dict.\n",
        "    data_limit=10,  # Amount of data used for validation. Integer between 5 and 100.\n",
        "    translation_source_field_name=\"\",  # Fill in with the corresponding field name of the source text in the data if translation metrics like Comet or MetricX are selected. Otherwise, leave it as empty.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd_uzQYQx6L7"
      },
      "source": [
        "#### Upload Vertex AI prompt optimizer Cloud Storage\n",
        "\n",
        "After you define Vertex AI prompt optimizer configuration, you upload them on Cloud Storage bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqiv8ApR_SAM"
      },
      "outputs": [],
      "source": [
        "args = vars(args)\n",
        "\n",
        "with epath.Path(CONFIG_FILE_URI).open(\"w\") as config_file:\n",
        "    json.dump(args, config_file)\n",
        "config_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqgBT8hYAle"
      },
      "source": [
        "#### Run the automatic prompt optimization job\n",
        "\n",
        "Now you are ready to run your first Vertex AI prompt optimizer job using the Vertex AI SDK for Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlNQ_UrFH9ne"
      },
      "source": [
        "> This prompt optimization job requires ~ 40 minutes to run.\n",
        "\n",
        "> Be sure you have provisioned enough queries per minute (QPM) quota implementing the recommended QPM for each model. If you configure the Vertex AI prompt optimizer with a QPM that is higher than the QPM than you have access to, the job might fail. [Check out](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-optimizer#before-you-begin) the documentation to know more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtPnvKIpUQ3q",
        "outputId": "c2c3457b-a721-46a5-bc2d-55bdb4975ead"
      },
      "outputs": [],
      "source": [
        "WORKER_POOL_SPECS = [\n",
        "    {\n",
        "        \"machine_spec\": {\n",
        "            \"machine_type\": \"n1-standard-4\",\n",
        "        },\n",
        "        \"replica_count\": 1,\n",
        "        \"container_spec\": {\n",
        "            \"image_uri\": APD_CONTAINER_URI,\n",
        "            \"args\": [\"--config=\" + CONFIG_FILE_URI],\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "custom_job = aiplatform.CustomJob(\n",
        "    display_name=PROMPT_OPTIMIZATION_JOB,\n",
        "    worker_pool_specs=WORKER_POOL_SPECS,\n",
        ")\n",
        "\n",
        "custom_job.submit(service_account=SERVICE_ACCOUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqfRaJ5iUVpP",
        "outputId": "1acc2122-7b23-4ce4-db04-b9e1676751f9"
      },
      "outputs": [],
      "source": [
        "custom_job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YwwKBhtJ4ut"
      },
      "source": [
        "### Collect and display the optimization results\n",
        "\n",
        "Vertex AI prompt optimizer returns both optimized templates and evaluation results for either instruction, or demostrations, or both depending on the optimization mode you define as JSONL files on Cloud Storage bucket. Those results help you understand the optimization process.\n",
        "\n",
        "In this case, you want to collect the optimized templates and evaluation results for the system instruction.\n",
        "\n",
        "Below you use a helper function to display those results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovqCR2u7QTZv",
        "outputId": "3af9d355-efc7-4e7a-f682-45952652564e"
      },
      "outputs": [],
      "source": [
        "print(OUTPUT_OPTIMIZATION_RUN_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765,
          "referenced_widgets": [
            "89bc2af98a37494aacc61e49cf3d096e",
            "830de867c25f44dcb7e3bb0ceebd3b88",
            "385ab94e8e724818aa06de706a057271",
            "b75af11d759042e392b82db38a77dee3",
            "ce9ba227c4b1456f8716470c980e1569",
            "4f7582a0bb664d93a1fba793f3d146c1",
            "54027884d8f647c4aa98469ed4cc33f4",
            "c6e40cddb8654f8993c71408c1665903",
            "b870a2a03b094c45b04e2f953e6a9e00",
            "09839a40e76b430a92cff51b1d2d7920",
            "2f6cd097abb94df99313b0a1d016b124",
            "e8bae5566be648d8aec5eec5eafc5b57",
            "ce9aa2cfcc8a453bb4aaa0f1ef3a7ace",
            "fb49fafeceec4a0fa0d424e9270796a5",
            "c730accbbde348189faa2f57556fc58d",
            "33ea9cd74ddb4fa2951d801a4e5d8143"
          ]
        },
        "id": "-6ohF0LJIhvU",
        "outputId": "01d18faa-120f-46e7-e0a4-9ee09c491b2e"
      },
      "outputs": [],
      "source": [
        "results_ui = vapo_lib.ResultsUI(OUTPUT_OPTIMIZATION_RUN_URI)\n",
        "results_df_html = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(results_df_html))\n",
        "display(results_ui.get_container())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## 4. Clean up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRY_3wh1GVNm"
      },
      "outputs": [],
      "source": [
        "delete_bucket = False\n",
        "delete_job = False\n",
        "delete_run = False\n",
        "delete_tutorial = False\n",
        "\n",
        "if delete_bucket:\n",
        "    ! gsutil rm -r {BUCKET_URI}\n",
        "\n",
        "if delete_job:\n",
        "    custom_job.delete()\n",
        "\n",
        "if delete_run:\n",
        "    ! gcloud functions delete 'custom_metric_case_2_2' --region={REGION}\n",
        "\n",
        "if delete_tutorial:\n",
        "    import shutil\n",
        "\n",
        "    shutil.rmtree(str(TUTORIAL_PATH))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2_vapo_sdk_custom_metric_multiple_choice_demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09839a40e76b430a92cff51b1d2d7920": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "200px"
          }
        },
        "2f6cd097abb94df99313b0a1d016b124": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33ea9cd74ddb4fa2951d801a4e5d8143": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "600px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": "20px 0px 0px 0px",
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": "auto",
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "385ab94e8e724818aa06de706a057271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "gs://redstone_base_bucket/optimization_case2_2/optimization_jobs/auto-prompt-design-job-jlgzqyy8/instruction"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_09839a40e76b430a92cff51b1d2d7920",
            "style": "IPY_MODEL_2f6cd097abb94df99313b0a1d016b124"
          }
        },
        "4f7582a0bb664d93a1fba793f3d146c1": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_33ea9cd74ddb4fa2951d801a4e5d8143",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<style type=\"text/css\">\n</style>\n<table id=\"T_37165\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_37165_level0_col0\" class=\"col_heading level0 col0\" >step</th>\n      <th id=\"T_37165_level0_col1\" class=\"col_heading level0 col1\" >prompt</th>\n      <th id=\"T_37165_level0_col2\" class=\"col_heading level0 col2\" >metrics.custom_engagement_personalization_score/mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_37165_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_37165_row0_col0\" class=\"data row0 col0\" ><div class=\"scrollable\">0</div></td>\n      <td id=\"T_37165_row0_col1\" class=\"data row0 col1\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_37165_row0_col2\" class=\"data row0 col2\" ><div class=\"scrollable\">0.4787</div></td>\n    </tr>\n  </tbody>\n</table>\n",
                  "text/plain": "<IPython.core.display.HTML object>"
                },
                "metadata": {},
                "output_type": "display_data"
              },
              {
                "name": "stdout",
                "output_type": "stream",
                "text": [
                  "\n"
                ]
              },
              {
                "data": {
                  "text/html": "<style type=\"text/css\">\n</style>\n<table id=\"T_f3fab\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_f3fab_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n      <th id=\"T_f3fab_level0_col1\" class=\"col_heading level0 col1\" >choices</th>\n      <th id=\"T_f3fab_level0_col2\" class=\"col_heading level0 col2\" >target</th>\n      <th id=\"T_f3fab_level0_col3\" class=\"col_heading level0 col3\" >prompt</th>\n      <th id=\"T_f3fab_level0_col4\" class=\"col_heading level0 col4\" >response</th>\n      <th id=\"T_f3fab_level0_col5\" class=\"col_heading level0 col5\" >reference</th>\n      <th id=\"T_f3fab_level0_col6\" class=\"col_heading level0 col6\" >custom_engagement_personalization_score/score</th>\n      <th id=\"T_f3fab_level0_col7\" class=\"col_heading level0 col7\" >custom_engagement_personalization_score/explanation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_f3fab_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_f3fab_row0_col0\" class=\"data row0 col0\" ><div class=\"scrollable\">In the context of military administration and logistics, what does the acronym \"ARM\" sometimes refer to?</div></td>\n      <td id=\"T_f3fab_row0_col1\" class=\"data row0 col1\" ><div class=\"scrollable\">[1] Advanced RISC Machine, [2] Army Reconnaissance Mission, [3] Automated Records Management, [4] Army Regulation Manual</div></td>\n      <td id=\"T_f3fab_row0_col2\" class=\"data row0 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":4,\n  \"reason\":\"While 'Advanced RISC Machine' is the most widely recognized meaning of ARM in technology, and other military-related interpretations exist, 'Army Regulation Manual' is a specific, though less common, usage within military administrative circles for official documentation and guidelines.\",\n  \"likelihood\":0.15\n}\n```</div></td>\n      <td id=\"T_f3fab_row0_col3\" class=\"data row0 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row0_col4\" class=\"data row0 col4\" ><div class=\"scrollable\">{\n  \"answer\": 3,\n  \"reason\": \"The question asks for the meaning of 'ARM' in the context of 'military administration and logistics'.\\n\\n1.  **Advanced RISC Machine**: This refers to a type of computer processor architecture. While used in military technology, it is not directly related to administrative or logistical functions.\\n2.  **Army Reconnaissance Mission**: This describes a military operational task, not an administrative or logistical process or system.\\n3.  **Automated Records Management**: This option fits perfectly within the specified context. 'Records management' is a core administrative function, and 'automated' systems are crucial for efficiency in modern military administration and logistics, handling vast amounts of data related to personnel, supplies, equipment, and operations.\\n4.  **Army Regulation Manual**: While regulations are fundamental to administration, the acronym 'ARM' is not a commonly recognized or standard abbreviation for 'Army Regulation Manual'. 'AR' is more typically used for 'Army Regulation'. 'Automated Records Management' is a more standard and applicable term in the context of modern administrative and logistical processes.\\n\\nTherefore, 'Automated Records Management' is the most appropriate and relevant interpretation of 'ARM' among the given choices for military administration and logistics.\",\n  \"likelihood\": 0.9\n}</div></td>\n      <td id=\"T_f3fab_row0_col5\" class=\"data row0 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":4,\n  \"reason\":\"While 'Advanced RISC Machine' is the most widely recognized meaning of ARM in technology, and other military-related interpretations exist, 'Army Regulation Manual' is a specific, though less common, usage within military administrative circles for official documentation and guidelines.\",\n  \"likelihood\":0.15\n}\n```</div></td>\n      <td id=\"T_f3fab_row0_col6\" class=\"data row0 col6\" ><div class=\"scrollable\">0.1</div></td>\n      <td id=\"T_f3fab_row0_col7\" class=\"data row0 col7\" ><div class=\"scrollable\">The model's performance was poor due to an incorrect answer choice and high confidence in that incorrect choice. 1. Matching Accuracy ($Score_M$): The model incorrectly identified 'Automated Records Management' (3) as the answer, while the ground truth was 'Army Regulation Manual' (4). This resulted in a score of 0.0. 2. Likelihood Accuracy ($Score_S$): The model predicted a likelihood of 0.9, while the ground truth likelihood was 0.15. The large absolute difference (0.75) yielded a low likelihood accuracy score of 0.25, indicating significant overconfidence in the wrong answer. 3. Reasoning Quality ($Score_R$): The model's reasoning provided a detailed analysis of all options, including the correct one (4), but ultimately dismissed it as 'not a commonly recognized or standard abbreviation' in favor of option 3. The ground truth reasoning, however, confirms 'Army Regulation Manual' as a specific, though less common, usage. While the model's explanation contains relevant keywords like 'Advanced RISC Machine' and 'Army Regulation Manual' and discusses the context of 'military administration', its core justification diverged significantly from the ground truth by leading to an incorrect conclusion. This resulted in an estimated ROUGE-L F-measure score of approximately 0.25, reflecting limited textual and conceptual alignment. Overall, the model failed to provide the correct answer, was highly confident in its incorrect choice, and its reasoning, while detailed, contained a critical misjudgment regarding the validity of the correct answer. The final score is calculated as (0.6 * 0.0) + (0.3 * 0.25) + (0.1 * 0.25) = 0.10.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_f3fab_row1_col0\" class=\"data row1 col0\" ><div class=\"scrollable\">In the field of advanced microscopy and nanoscience, what does the acronym SNOM stand for?</div></td>\n      <td id=\"T_f3fab_row1_col1\" class=\"data row1 col1\" ><div class=\"scrollable\">[1] Standard Nanoscale Optical Module, [2] Scanning Near-field Optical Microscopy, [3] Super-resolution Nano-Observation Machine, [4] Spatial Nanospectroscopy of Organic Materials</div></td>\n      <td id=\"T_f3fab_row1_col2\" class=\"data row1 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":2,\n  \"reason\":\"SNOM stands for Scanning Near-field Optical Microscopy, a technique used to achieve spatial resolution beyond the diffraction limit of light by probing the sample with an evanescent field.\",\n  \"likelihood\":0.05\n}\n```</div></td>\n      <td id=\"T_f3fab_row1_col3\" class=\"data row1 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row1_col4\" class=\"data row1 col4\" ><div class=\"scrollable\">{\n  \"answer\": 2,\n  \"reason\": \"Scanning Near-field Optical Microscopy (SNOM) is a widely recognized and standard acronym in the field of advanced microscopy and nanoscience. The other options are either less accurate in their terminology (e.g., 'Module', 'Machine') or describe a subset/characteristic rather than the full, established name of the technique.\",\n  \"likelihood\": 1.0\n}</div></td>\n      <td id=\"T_f3fab_row1_col5\" class=\"data row1 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":2,\n  \"reason\":\"SNOM stands for Scanning Near-field Optical Microscopy, a technique used to achieve spatial resolution beyond the diffraction limit of light by probing the sample with an evanescent field.\",\n  \"likelihood\":0.05\n}\n```</div></td>\n      <td id=\"T_f3fab_row1_col6\" class=\"data row1 col6\" ><div class=\"scrollable\">0.682</div></td>\n      <td id=\"T_f3fab_row1_col7\" class=\"data row1 col7\" ><div class=\"scrollable\">The model's prediction successfully matched the ground truth answer (2), resulting in a perfect matching accuracy score ($Score_M = 1.0$). However, the predicted likelihood (1.0) was significantly higher than the ground truth likelihood (0.05), leading to a very low likelihood accuracy score ($Score_S = 0.05$). The reasoning provided shared the core definition of 'Scanning Near-field Optical Microscopy (SNOM)' with the ground truth, but diverged in the elaborative details, yielding a moderate reasoning quality score ($Score_R = 0.67$). The final score is calculated as (0.6 * 1.0) + (0.3 * 0.05) + (0.1 * 0.67) = 0.6 + 0.015 + 0.067 = 0.682.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_f3fab_row2_col0\" class=\"data row2 col0\" ><div class=\"scrollable\">In the field of Materials Science, what does SEM-EDS stand for?</div></td>\n      <td id=\"T_f3fab_row2_col1\" class=\"data row2 col1\" ><div class=\"scrollable\">[1] Scanning Electron Microscopy - Energy-Dispersive Spectroscopy, [2] Surface Engineering Manufacturing - Electron Diffraction System, [3] Solid-state Electronic Materials - Environmental Degradation Studies, [4] Spatial Electron Mapping - Element Detection System</div></td>\n      <td id=\"T_f3fab_row2_col2\" class=\"data row2 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":1,\n  \"reason\":\"SEM-EDS is a common analytical technique in materials science where Scanning Electron Microscopy (SEM) is combined with Energy-Dispersive X-ray Spectroscopy (EDS or EDX) for elemental analysis and imaging of material surfaces.\",\n  \"likelihood\":0.25\n}\n```</div></td>\n      <td id=\"T_f3fab_row2_col3\" class=\"data row2 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row2_col4\" class=\"data row2 col4\" ><div class=\"scrollable\">{\n  \"answer\": 1,\n  \"reason\": \"In the field of Materials Science, SEM is a universally recognized acronym for Scanning Electron Microscopy, a fundamental technique for surface imaging and analysis. EDS (often coupled with SEM) stands for Energy-Dispersive Spectroscopy, which is used for elemental analysis. Option [1] provides the exact and standard full forms for both acronyms in this context. The other options present expansions that are not standard or correct within Materials Science.\",\n  \"likelihood\": 1.0\n}</div></td>\n      <td id=\"T_f3fab_row2_col5\" class=\"data row2 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":1,\n  \"reason\":\"SEM-EDS is a common analytical technique in materials science where Scanning Electron Microscopy (SEM) is combined with Energy-Dispersive X-ray Spectroscopy (EDS or EDX) for elemental analysis and imaging of material surfaces.\",\n  \"likelihood\":0.25\n}\n```</div></td>\n      <td id=\"T_f3fab_row2_col6\" class=\"data row2 col6\" ><div class=\"scrollable\">0.74</div></td>\n      <td id=\"T_f3fab_row2_col7\" class=\"data row2 col7\" ><div class=\"scrollable\">The `r_answer` of '1' perfectly matched the `answer` of '1', resulting in a Matching Accuracy (Score_M) of 1.0. The `r_likelihood` of '1.0' was significantly different from the `likelihood` of '0.25', leading to a Likelihood Accuracy (Score_S) of 0.25 (1.0 - |1.0 - 0.25|). The `r_reason` provided relevant information about Scanning Electron Microscopy (SEM) and Energy-Dispersive Spectroscopy (EDS) in Materials Science, aligning well with the ground truth `reason` which also described SEM-EDS in the context of materials analysis. This resulted in an estimated Reasoning Quality (ROUGE-L F-measure, Score_R) of approximately 0.65. The final score is calculated as (0.6 * 1.0) + (0.3 * 0.25) + (0.1 * 0.65) = 0.6 + 0.075 + 0.065 = 0.74.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_f3fab_row3_col0\" class=\"data row3 col0\" ><div class=\"scrollable\">In the context of environmental science and conservation, what does the acronym 'DDT' infrequently refer to?</div></td>\n      <td id=\"T_f3fab_row3_col1\" class=\"data row3 col1\" ><div class=\"scrollable\">[1] Dichlorodiphenyltrichloroethane, [2] Database for Dangerous Toxins, [3] Deep-Sea Drilling Technology, [4] Decline of Diverse Terrestrial ecosystems</div></td>\n      <td id=\"T_f3fab_row3_col2\" class=\"data row3 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":4,\n  \"reason\":\"While DDT is overwhelmingly recognized as Dichlorodiphenyltrichloroethane, an insecticide, in certain niche environmental discussions, particularly related to broad ecological trends, it has been used as an unpopular shorthand for 'Decline of Diverse Terrestrial ecosystems' to highlight a specific conservation issue, though this usage is very rare and not standardized.\",\n  \"likelihood\":0.05\n}\n```</div></td>\n      <td id=\"T_f3fab_row3_col3\" class=\"data row3 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row3_col4\" class=\"data row3 col4\" ><div class=\"scrollable\">{\n  \"answer\": 3,\n  \"reason\": \"Dichlorodiphenyltrichloroethane is the widely recognized and primary meaning of 'DDT' in environmental science and conservation. The question asks for an *infrequent* reference, meaning something that is rarely or never associated with the acronym. Option [1] is the frequent and primary meaning, so it cannot be the answer. Options [2], [3], and [4] are not standard or widely recognized meanings of 'DDT'. Among these non-standard options, 'Deep-Sea Drilling Technology' ([3]) is the least plausible and least relevant interpretation within the specific context of 'environmental science and conservation' compared to 'Database for Dangerous Toxins' or 'Decline of Diverse Terrestrial ecosystems'. Therefore, it represents the most 'infrequent' or least likely alternative reference for 'DDT' in this field.\",\n  \"likelihood\": 0.01\n}</div></td>\n      <td id=\"T_f3fab_row3_col5\" class=\"data row3 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":4,\n  \"reason\":\"While DDT is overwhelmingly recognized as Dichlorodiphenyltrichloroethane, an insecticide, in certain niche environmental discussions, particularly related to broad ecological trends, it has been used as an unpopular shorthand for 'Decline of Diverse Terrestrial ecosystems' to highlight a specific conservation issue, though this usage is very rare and not standardized.\",\n  \"likelihood\":0.05\n}\n```</div></td>\n      <td id=\"T_f3fab_row3_col6\" class=\"data row3 col6\" ><div class=\"scrollable\">0.323</div></td>\n      <td id=\"T_f3fab_row3_col7\" class=\"data row3 col7\" ><div class=\"scrollable\">The evaluation yielded an 'eval_score' of 0.323 based on the following breakdown:\n-   **Matching Accuracy (Weighted 60%):** The model's predicted answer (3) did not match the ground truth answer (4), resulting in a score of 0.0 for this component. This significantly impacted the overall score.\n-   **Likelihood Accuracy (Weighted 30%):** The model's predicted likelihood (0.01) was very close to the ground truth likelihood (0.05). The absolute difference is 0.04, leading to a high score of 1.0 - 0.04 = 0.96 for this component.\n-   **Reasoning Quality (Weighted 10%):** The model's reasoning discussed the primary meaning of 'DDT' and the need for an 'infrequent' reference, aligning with the ground truth's initial premise. However, the model's specific justification for selecting option 3 as the least plausible infrequent reference diverged from the ground truth's explanation for option 4. The ROUGE-L F-measure for the reasoning was approximately 0.35, reflecting some shared understanding of the problem but different specific conclusions.\n\n**Overall Score Calculation:** (0.6 * 0.0) + (0.3 * 0.96) + (0.1 * 0.35) = 0.0 + 0.288 + 0.035 = 0.323.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_f3fab_row4_col0\" class=\"data row4 col0\" ><div class=\"scrollable\">While commonly known in business as 'Enterprise Resource Planning,' the acronym ERP has a distinct, less widely known meaning in another scientific field. What does ERP refer to in the context of cognitive neuroscience?</div></td>\n      <td id=\"T_f3fab_row4_col1\" class=\"data row4 col1\" ><div class=\"scrollable\">[1] Enterprise Resource Planning, [2] Event Related Potentials, [3] Electronic Research Protocols, [4] Experimental Reaction Pathways</div></td>\n      <td id=\"T_f3fab_row4_col2\" class=\"data row4 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":2,\n  \"reason\":\"ERP is overwhelmingly recognized as 'Enterprise Resource Planning' in the business world. However, in cognitive neuroscience, ERP stands for 'Event Related Potentials,' which are measured brain responses directly related to a specific event. This meaning is highly specialized and not widely known outside of neuroscience, making it an 'unpopular' or less common general understanding of the acronym.\",\n  \"likelihood\":0.07\n}\n```</div></td>\n      <td id=\"T_f3fab_row4_col3\" class=\"data row4 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row4_col4\" class=\"data row4 col4\" ><div class=\"scrollable\">{\n  \"answer\": 2,\n  \"reason\": \"The question specifically asks for the meaning of ERP in the context of cognitive neuroscience, distinguishing it from its common business meaning. 'Event Related Potentials' (ERP) is a fundamental and widely recognized term in cognitive neuroscience, referring to measured brain responses that are time-locked to specific sensory, motor, or cognitive events. The other options are either the incorrect business meaning, not standard terms in the field, or belong to different scientific disciplines.\",\n  \"likelihood\": 1.0\n}</div></td>\n      <td id=\"T_f3fab_row4_col5\" class=\"data row4 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":2,\n  \"reason\":\"ERP is overwhelmingly recognized as 'Enterprise Resource Planning' in the business world. However, in cognitive neuroscience, ERP stands for 'Event Related Potentials,' which are measured brain responses directly related to a specific event. This meaning is highly specialized and not widely known outside of neuroscience, making it an 'unpopular' or less common general understanding of the acronym.\",\n  \"likelihood\":0.07\n}\n```</div></td>\n      <td id=\"T_f3fab_row4_col6\" class=\"data row4 col6\" ><div class=\"scrollable\">0.6960000000000001</div></td>\n      <td id=\"T_f3fab_row4_col7\" class=\"data row4 col7\" ><div class=\"scrollable\">The model achieved a perfect score for matching accuracy as its predicted answer (2) exactly matched the ground truth answer (2). However, the likelihood accuracy was very low; the model predicted a likelihood of 1.0, while the ground truth was 0.07, indicating significant overconfidence. The reasoning quality was good; the model's explanation correctly identified 'Event Related Potentials' in cognitive neuroscience and distinguished it from the business meaning, aligning well with the ground truth's explanation, despite differences in phrasing. The holistic score is primarily boosted by the correct answer but significantly pulled down by the inaccurate likelihood.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_f3fab_row5_col0\" class=\"data row5 col0\" ><div class=\"scrollable\">What does the acronym DNS primarily stand for in the specialized field of physical therapy and rehabilitation?</div></td>\n      <td id=\"T_f3fab_row5_col1\" class=\"data row5 col1\" ><div class=\"scrollable\">[1] Dynamic Neuromuscular Stabilization, [2] Domain Name System, [3] Data Network Security, [4] Digital Neuro-Sensor</div></td>\n      <td id=\"T_f3fab_row5_col2\" class=\"data row5 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":1,\n  \"reason\":\"While DNS is overwhelmingly recognized as 'Domain Name System' in computer science and networking, its application in physical therapy and rehabilitation refers to 'Dynamic Neuromuscular Stabilization', a method used to assess and restore the locomotor system. This latter meaning is far less known to the general public or even professionals outside of that specific medical sub-discipline, making it an 'unpopular' or niche meaning of the acronym.\",\n  \"likelihood\":0.07\n}\n```</div></td>\n      <td id=\"T_f3fab_row5_col3\" class=\"data row5 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row5_col4\" class=\"data row5 col4\" ><div class=\"scrollable\">{\n  \"answer\": 1,\n  \"reason\": \"The question specifically asks for the meaning of DNS within the 'specialized field of physical therapy and rehabilitation'. 'Dynamic Neuromuscular Stabilization' (DNS) is a well-established and recognized therapeutic approach and concept in this field. The other options, 'Domain Name System' and 'Data Network Security', are common meanings of DNS in computer science and IT, which are entirely unrelated to physical therapy. 'Digital Neuro-Sensor' is not a primary or widely recognized meaning of DNS in this context.\",\n  \"likelihood\": 0.99\n}</div></td>\n      <td id=\"T_f3fab_row5_col5\" class=\"data row5 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":1,\n  \"reason\":\"While DNS is overwhelmingly recognized as 'Domain Name System' in computer science and networking, its application in physical therapy and rehabilitation refers to 'Dynamic Neuromuscular Stabilization', a method used to assess and restore the locomotor system. This latter meaning is far less known to the general public or even professionals outside of that specific medical sub-discipline, making it an 'unpopular' or niche meaning of the acronym.\",\n  \"likelihood\":0.07\n}\n```</div></td>\n      <td id=\"T_f3fab_row5_col6\" class=\"data row5 col6\" ><div class=\"scrollable\">0.709</div></td>\n      <td id=\"T_f3fab_row5_col7\" class=\"data row5 col7\" ><div class=\"scrollable\">The model correctly identified the answer ('1') matching the ground truth, contributing a perfect score to matching accuracy. The reasoning provided by the model was also of good quality, effectively explaining why 'Dynamic Neuromuscular Stabilization' is the correct meaning of DNS in physical therapy and contrasting it with other interpretations, resulting in a high reasoning score. However, the model predicted a very high likelihood of '0.99', whereas the ground truth likelihood was a very low '0.07'. This significant discrepancy in likelihood resulted in a very poor likelihood accuracy score, which heavily penalized the overall evaluation despite the correct answer and good reasoning.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_f3fab_row6_col0\" class=\"data row6 col0\" ><div class=\"scrollable\">In the field of networking and telecommunications, what does the acronym MPLS most commonly refer to, outside of its primary 'Multiprotocol Label Switching' usage?</div></td>\n      <td id=\"T_f3fab_row6_col1\" class=\"data row6 col1\" ><div class=\"scrollable\">[1] Multiprotocol Label Switching, [2] Minimum Packet Loss System, [3] Multi-Platform Load Sharing, [4] Mobile Positioning and Location Services</div></td>\n      <td id=\"T_f3fab_row6_col2\" class=\"data row6 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":4,\n  \"reason\":\"While MPLS is overwhelmingly known as 'Multiprotocol Label Switching' in core networking, the acronym also less commonly stands for 'Mobile Positioning and Location Services' in certain contexts within mobile telecommunications, particularly in older 2G\\/3G systems or related research.\",\n  \"likelihood\":0.05\n}\n```</div></td>\n      <td id=\"T_f3fab_row6_col3\" class=\"data row6 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row6_col4\" class=\"data row6 col4\" ><div class=\"scrollable\">{\n  \"answer\": 2,\n  \"reason\": \"The question asks for what MPLS most commonly refers to, outside of its primary 'Multiprotocol Label Switching' usage. In the field of networking and telecommunications, MPLS almost exclusively refers to 'Multiprotocol Label Switching'. There are no other widely recognized or common meanings for MPLS in this domain that match any of the provided choices (2, 3, or 4). Option 1 is explicitly excluded by the question's premise. Therefore, the question is flawed as none of the alternative options are commonly referred to as MPLS. Since a choice must be made, and all alternative options are incorrect regarding common usage, option 2 ('Minimum Packet Loss System') is chosen arbitrarily to fulfill the format requirement, but it is not a common meaning for MPLS. The likelihood is extremely low because this meaning is not associated with MPLS in common usage within networking and telecommunications.\",\n  \"likelihood\": 0.01\n}</div></td>\n      <td id=\"T_f3fab_row6_col5\" class=\"data row6 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":4,\n  \"reason\":\"While MPLS is overwhelmingly known as 'Multiprotocol Label Switching' in core networking, the acronym also less commonly stands for 'Mobile Positioning and Location Services' in certain contexts within mobile telecommunications, particularly in older 2G\\/3G systems or related research.\",\n  \"likelihood\":0.05\n}\n```</div></td>\n      <td id=\"T_f3fab_row6_col6\" class=\"data row6 col6\" ><div class=\"scrollable\">0.298</div></td>\n      <td id=\"T_f3fab_row6_col7\" class=\"data row6 col7\" ><div class=\"scrollable\">The model's performance was mixed. For **Matching Accuracy (weight 60%)**, the predicted answer (2) did not match the ground truth (4), resulting in a score of 0.0. For **Likelihood Accuracy (weight 30%)**, the predicted likelihood of 0.01 was very close to the ground truth likelihood of 0.05, yielding a high score of 0.96. For **Reasoning Quality (weight 10%)**, the model's explanation diverged significantly from the ground truth. The model claimed no other common meanings exist and arbitrarily picked an answer, while the ground truth provided a specific less common meaning ('Mobile Positioning and Location Services'). This factual discrepancy and lack of direct overlap led to a very low estimated ROUGE-L F-measure score of 0.10. The final `eval_score` is calculated as (0.6 * 0.0) + (0.3 * 0.96) + (0.1 * 0.10) = 0.0 + 0.288 + 0.01 = 0.298.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_f3fab_row7_col0\" class=\"data row7 col0\" ><div class=\"scrollable\">In the highly specialized field of electrical power systems and grid management, what does the acronym SOP stand for?</div></td>\n      <td id=\"T_f3fab_row7_col1\" class=\"data row7 col1\" ><div class=\"scrollable\">[1] Standard Operating Procedure, [2] System Optimization Protocol, [3] Sequence of Operations, [4] Systems of Protection</div></td>\n      <td id=\"T_f3fab_row7_col2\" class=\"data row7 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":4,\n  \"reason\":\"While 'Standard Operating Procedure' is the most widely recognized meaning for SOP across many industries, in the specific domain of electrical engineering and power grids, SOP often refers to 'Systems of Protection', which are critical components designed to detect and isolate faults to ensure grid stability and reliability. This usage is highly specialized and not commonly known outside the field.\",\n  \"likelihood\":0.15\n}\n```</div></td>\n      <td id=\"T_f3fab_row7_col3\" class=\"data row7 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row7_col4\" class=\"data row7 col4\" ><div class=\"scrollable\">{\n  \"answer\": 1,\n  \"reason\": \"The acronym SOP most commonly stands for 'Standard Operating Procedure' across a vast array of industries, including highly specialized fields like electrical power systems and grid management. In this context, SOPs are essential for defining consistent, safe, and efficient methods for various tasks, from routine maintenance to emergency response and operational protocols. While 'Sequence of Operations' is a relevant concept in power system control and automation, 'Standard Operating Procedure' is the most universally recognized and frequently applied meaning of the acronym SOP within the broader scope of grid management.\",\n  \"likelihood\": 0.95\n}</div></td>\n      <td id=\"T_f3fab_row7_col5\" class=\"data row7 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":4,\n  \"reason\":\"While 'Standard Operating Procedure' is the most widely recognized meaning for SOP across many industries, in the specific domain of electrical engineering and power grids, SOP often refers to 'Systems of Protection', which are critical components designed to detect and isolate faults to ensure grid stability and reliability. This usage is highly specialized and not commonly known outside the field.\",\n  \"likelihood\":0.15\n}\n```</div></td>\n      <td id=\"T_f3fab_row7_col6\" class=\"data row7 col6\" ><div class=\"scrollable\">0.1</div></td>\n      <td id=\"T_f3fab_row7_col7\" class=\"data row7 col7\" ><div class=\"scrollable\">Step 0: Input Parsing\n- r_answer: Parsed as integer 1\n- r_likelihood: Parsed as float 0.95\n- answer: Parsed as integer 4\n- likelihood: Parsed as float 0.15\n\nStep 1: Matching Accuracy (Score_M)\n- The predicted answer (1) does not match the ground truth answer (4).\n- Score_M = 0.0\n\nStep 2: Likelihood Accuracy (Score_S)\n- Predicted likelihood = 0.95, Ground truth likelihood = 0.15\n- Absolute difference = |0.95 - 0.15| = 0.80\n- Score_S = 1.0 - 0.80 = 0.20\n\nStep 3: Reasoning Quality (Score_R)\n- The model's reasoning explained that 'Standard Operating Procedure' is the most common meaning of SOP, even in 'electrical power systems and grid management'. This explanation is well-structured and plausible for a general context. However, the ground truth specifically identifies 'Systems of Protection' as the meaning often referred to in the 'specific domain of electrical engineering and power grids', highlighting a specialized context. The model's reason directly asserts that 'Standard Operating Procedure' is the most universally recognized meaning *within the broader scope of grid management*, which conflicts with the ground truth's specific domain emphasis. While there's lexical overlap concerning the general meaning of SOP and its presence in relevant fields, the model missed the crucial domain-specific alternative and nuance. Based on ROUGE-L F-measure, the score is estimated to be 0.40.\n\nStep 4: Final Score Calculation\n- Final Score = (0.6 * Score_M) + (0.3 * Score_S) + (0.1 * Score_R)\n- Final Score = (0.6 * 0.0) + (0.3 * 0.20) + (0.1 * 0.40)\n- Final Score = 0.0 + 0.06 + 0.04\n- Final Score = 0.10</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_f3fab_row8_col0\" class=\"data row8 col0\" ><div class=\"scrollable\">In the field of biochemical engineering, what does the acronym `HPLC` stand for?</div></td>\n      <td id=\"T_f3fab_row8_col1\" class=\"data row8 col1\" ><div class=\"scrollable\">[1] Highly Permeable Liquid Culture, [2] High-Performance Liquid Chromatography, [3] Homogeneous Polymerization Loop Chamber, [4] Hybrid Protein Ligation Complex</div></td>\n      <td id=\"T_f3fab_row8_col2\" class=\"data row8 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":2,\n  \"reason\":\"HPLC is a widely used analytical chemistry technique used to separate, identify, and quantify each component in a mixture. It stands for High-Performance Liquid Chromatography (formerly High-Pressure Liquid Chromatography).\",\n  \"likelihood\":0.35\n}\n```</div></td>\n      <td id=\"T_f3fab_row8_col3\" class=\"data row8 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row8_col4\" class=\"data row8 col4\" ><div class=\"scrollable\">{\n  \"answer\": 2,\n  \"reason\": \"HPLC is a standard and universally recognized acronym in biochemical engineering and analytical chemistry, standing for High-Performance Liquid Chromatography. This technique is widely used for separating, identifying, and quantifying compounds. The other choices do not represent the established meaning of HPLC.\",\n  \"likelihood\": 1.0\n}</div></td>\n      <td id=\"T_f3fab_row8_col5\" class=\"data row8 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":2,\n  \"reason\":\"HPLC is a widely used analytical chemistry technique used to separate, identify, and quantify each component in a mixture. It stands for High-Performance Liquid Chromatography (formerly High-Pressure Liquid Chromatography).\",\n  \"likelihood\":0.35\n}\n```</div></td>\n      <td id=\"T_f3fab_row8_col6\" class=\"data row8 col6\" ><div class=\"scrollable\">0.79</div></td>\n      <td id=\"T_f3fab_row8_col7\" class=\"data row8 col7\" ><div class=\"scrollable\">The model's answer is perfectly accurate, with 'r_answer' (2) matching 'answer' (2), resulting in a Matching Accuracy (Score_M) of 1.0. However, the predicted likelihood 'r_likelihood' (1.0) deviates significantly from the ground truth 'likelihood' (0.35), leading to a Likelihood Accuracy (Score_S) of 0.35. The reasoning provided by the model is highly similar to the ground truth, yielding a Reasoning Quality (Score_R) of 0.85. The final score is calculated as (0.6 * 1.0) + (0.3 * 0.35) + (0.1 * 0.85) = 0.6 + 0.105 + 0.085 = 0.79.</div></td>\n    </tr>\n    <tr>\n      <th id=\"T_f3fab_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_f3fab_row9_col0\" class=\"data row9 col0\" ><div class=\"scrollable\">In the specialized field of materials science, particularly concerning composites, what does the acronym 'SMC' *uncommonly* refer to?</div></td>\n      <td id=\"T_f3fab_row9_col1\" class=\"data row9 col1\" ><div class=\"scrollable\">[1] Sheet Molding Compound, [2] Silicon Micro-Channel, [3] Structural Material Classification, [4] Surface Modification Coating</div></td>\n      <td id=\"T_f3fab_row9_col2\" class=\"data row9 col2\" ><div class=\"scrollable\">```json\n{\n  \"answer\":2,\n  \"reason\":\"While 'Sheet Molding Compound' ([1]) is the most common and widely recognized meaning of SMC in materials science and manufacturing, and 'Structural Material Classification' ([3]) and 'Surface Modification Coating' ([4]) are plausible but generally not standard acronyms, 'Silicon Micro-Channel' ([2]) is a lesser-known but valid usage of SMC, specifically encountered in niche research areas like microfluidics or micro-electromechanical systems (MEMS) where silicon structures are prevalent. This usage is significantly less prevalent than the 'Sheet Molding Compound' definition.\",\n  \"likelihood\":0.1\n}\n```</div></td>\n      <td id=\"T_f3fab_row9_col3\" class=\"data row9 col3\" ><div class=\"scrollable\">\nYou are a smart multiple-choice answering assistant.\nYou're role is to choose right answer of given \"question\", among given \"choices\", which are possible choices.\n\nPlease return a JSON object with three properties: \"answer\", \"reason\" and \"likelihood\".\nThe \"answer\" property is a integer, indicating the right choice number.\nThe \"reason\" property is a string that explains which of the comparisons are the best matches\nfor the input, and the reason should also explain any factors that influence the likelihood score.\nThe \"likelihood\" is a probability that in reality, the acronym is used for that meaning, and must be between 0~1.\n</div></td>\n      <td id=\"T_f3fab_row9_col4\" class=\"data row9 col4\" ><div class=\"scrollable\">{\n  \"answer\": 4,\n  \"reason\": \"The question asks for an *uncommon* reference for the acronym 'SMC' in materials science, particularly concerning composites. \\n\\n1.  **Sheet Molding Compound (SMC)** is the most common and widely recognized meaning of SMC in the field of composites. Therefore, it cannot be the answer to what SMC *uncommonly* refers to.\\n2.  **Structural Material Classification (SMC)** is a generic descriptive phrase and is not a recognized or established acronym for SMC in materials science or any other field. Its likelihood of being an actual reference is extremely low.\\n3.  **Silicon Micro-Channel (SMC)** is a valid acronym in other specialized fields like microfluidics or MEMS. While composites can incorporate silicon, 'SMC' is not a common or established acronym for 'Silicon Micro-Channel' within the specialized field of composites. Its use in this context would be uncommon, but it's an acronym from a different primary domain.\\n4.  **Surface Modification Coating (SMC)**: While 'SMC' is not a universally standard acronym for this general term, specific types of surface modification coatings are occasionally abbreviated as SMC in niche scientific literature. For example, 'Self-assembled Monolayer Coating' is sometimes abbreviated as SMC. Since 'Self-assembled Monolayer Coating' is a specific type of surface modification coating, this makes 'Surface Modification Coating' a plausible, albeit *uncommon*, reference for SMC within materials science, including composites. It is conceptually closer to the general field of materials science and has a more direct, albeit niche, acronymic link compared to 'Silicon Micro-Channel' in the context of composites.\\n\\nTherefore, 'Surface Modification Coating' is the best choice for an uncommon reference, as it encompasses niche but existing uses of SMC for related concepts within materials science.\",\n  \"likelihood\": 0.08\n}</div></td>\n      <td id=\"T_f3fab_row9_col5\" class=\"data row9 col5\" ><div class=\"scrollable\">```json\n{\n  \"answer\":2,\n  \"reason\":\"While 'Sheet Molding Compound' ([1]) is the most common and widely recognized meaning of SMC in materials science and manufacturing, and 'Structural Material Classification' ([3]) and 'Surface Modification Coating' ([4]) are plausible but generally not standard acronyms, 'Silicon Micro-Channel' ([2]) is a lesser-known but valid usage of SMC, specifically encountered in niche research areas like microfluidics or micro-electromechanical systems (MEMS) where silicon structures are prevalent. This usage is significantly less prevalent than the 'Sheet Molding Compound' definition.\",\n  \"likelihood\":0.1\n}\n```</div></td>\n      <td id=\"T_f3fab_row9_col6\" class=\"data row9 col6\" ><div class=\"scrollable\">0.34900000000000003</div></td>\n      <td id=\"T_f3fab_row9_col7\" class=\"data row9 col7\" ><div class=\"scrollable\">The model's performance was significantly impacted by an incorrect answer, despite good likelihood proximity and partially relevant reasoning. The matching accuracy score was 0.0 because the predicted answer '4' did not match the ground truth answer '2'. The likelihood accuracy score was 0.98, reflecting the model's predicted likelihood of 0.08 being very close to the ground truth of 0.1. The reasoning quality score was approximately 0.55. While the model provided a detailed analysis of all options, correctly identifying 'Sheet Molding Compound' as common and discussing 'Silicon Micro-Channel' in a manner somewhat consistent with the ground truth's description (valid in other specialized fields like microfluidics/MEMS), its ultimate conclusion for the 'best uncommon' reference ('Surface Modification Coating') differed from the ground truth's selection of 'Silicon Micro-Channel'.</div></td>\n    </tr>\n  </tbody>\n</table>\n",
                  "text/plain": "<IPython.core.display.HTML object>"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "54027884d8f647c4aa98469ed4cc33f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830de867c25f44dcb7e3bb0ceebd3b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6e40cddb8654f8993c71408c1665903",
            "placeholder": "​",
            "style": "IPY_MODEL_b870a2a03b094c45b04e2f953e6a9e00",
            "value": "Select Run:"
          }
        },
        "89bc2af98a37494aacc61e49cf3d096e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_830de867c25f44dcb7e3bb0ceebd3b88",
              "IPY_MODEL_385ab94e8e724818aa06de706a057271",
              "IPY_MODEL_b75af11d759042e392b82db38a77dee3",
              "IPY_MODEL_ce9ba227c4b1456f8716470c980e1569",
              "IPY_MODEL_4f7582a0bb664d93a1fba793f3d146c1"
            ],
            "layout": "IPY_MODEL_54027884d8f647c4aa98469ed4cc33f4"
          }
        },
        "b75af11d759042e392b82db38a77dee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8bae5566be648d8aec5eec5eafc5b57",
            "placeholder": "​",
            "style": "IPY_MODEL_ce9aa2cfcc8a453bb4aaa0f1ef3a7ace",
            "value": "Select Template:"
          }
        },
        "b870a2a03b094c45b04e2f953e6a9e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6e40cddb8654f8993c71408c1665903": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c730accbbde348189faa2f57556fc58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce9aa2cfcc8a453bb4aaa0f1ef3a7ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce9ba227c4b1456f8716470c980e1569": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DropdownModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Template 0 custom_engagement_personalization_score: 0.4787",
              "Template 1 custom_engagement_personalization_score: 0.5932000000000001",
              "Template 2 custom_engagement_personalization_score: 0.6314299999999999",
              "Template 3 custom_engagement_personalization_score: 0.6434",
              "Template 4 custom_engagement_personalization_score: 0.5279360000000001"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_fb49fafeceec4a0fa0d424e9270796a5",
            "style": "IPY_MODEL_c730accbbde348189faa2f57556fc58d"
          }
        },
        "e8bae5566be648d8aec5eec5eafc5b57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb49fafeceec4a0fa0d424e9270796a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "400px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
