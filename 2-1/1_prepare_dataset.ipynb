{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63dc84b5",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "\n",
    "ds = load_dataset(\"google/civil_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3e40eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 1804874\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 97320\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
      "        num_rows: 97320\n",
      "    })\n",
      "})\n",
      "{'text': \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\", 'toxicity': 0.0, 'severe_toxicity': 0.0, 'obscene': 0.0, 'threat': 0.0, 'insult': 0.0, 'identity_attack': 0.0, 'sexual_explicit': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8c6e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 8)\n",
      "(97320, 8)\n",
      "(97320, 8)\n"
     ]
    }
   ],
   "source": [
    "train_df = ds['train'].to_pandas()\n",
    "valid_df = ds['validation'].to_pandas()\n",
    "test_df = ds['test'].to_pandas()\n",
    "\n",
    "print(train_df.shape)\n",
    "print(valid_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0bfc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult',\n",
      "       'identity_attack', 'sexual_explicit'],\n",
      "      dtype='object')\n",
      "## Row counts\n",
      "(1000, 8)\n",
      "(300, 8)\n"
     ]
    }
   ],
   "source": [
    "def random_sample_n(df, n):\n",
    "    df['rand'] = np.random.randint(0, 30)\n",
    "    df_new = df.sort_values(by='rand').reset_index(drop=True).drop('rand', axis=1)\n",
    "    return df_new.iloc[:n]\n",
    "\n",
    "train_df_sample = random_sample_n(train_df, 1000)\n",
    "test_df_sample = random_sample_n(test_df, 300)\n",
    "\n",
    "print(train_df_sample.columns)\n",
    "\n",
    "print(\"## Row counts\")\n",
    "print(train_df_sample.shape)\n",
    "print(test_df_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "004e764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult',\n",
    "       'identity_attack', 'sexual_explicit']\n",
    "\n",
    "train_df_sample[label_columns] = train_df_sample[label_columns].round(3)\n",
    "test_df_sample[label_columns] = test_df_sample[label_columns].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d64178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_formatted_json(row):\n",
    "    json_content = row.to_json(orient='index', double_precision=2, indent=2)\n",
    "    \n",
    "    formatted_string = '```json\\n' + json_content + '\\n```'\n",
    "    \n",
    "    return formatted_string\n",
    "\n",
    "train_df_sample['response'] = train_df_sample[label_columns].apply(row_to_formatted_json, axis=1)\n",
    "test_df_sample['response'] = test_df_sample[label_columns].apply(row_to_formatted_json, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b66d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = '''\n",
    "Analyze the following text for toxicity and provide scores for the specified categories. Each score must be a float between 0.0 and 1.0. Your response must be only a single, valid JSON object with the specified keys, formatted exactly as shown below, including the enclosing '```json\\n...\\n```' block.\n",
    "\n",
    "Text: {text_input}\n",
    "\n",
    "Output format:\n",
    "'```json\\n{{\\n  \"toxicity\":<float_score>,\\n  \"severe_toxicity\":<float_score>,\\n  \"obscene\":<float_score>,\\n  \"threat\":<float_score>,\\n  \"insult\":<float_score>,\\n  \"identity_attack\":<float_score>,\\n  \"sexual_explicit\":<float_score>\\n}}\\n```'\n",
    "'''\n",
    "\n",
    "train_df_sample['prompt'] = train_df_sample['text'].apply(lambda x: initial_prompt.format(text_input=x))\n",
    "test_df_sample['prompt'] = test_df_sample['text'].apply(lambda x: initial_prompt.format(text_input=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "223ec7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_sample.to_csv('./sample_data/train_df_sampled.csv', index=False)\n",
    "test_df_sample.to_csv('./sample_data/test_df_sampled.csv', index=False)\n",
    "\n",
    "train_df_sample[['text', 'response']].to_csv('./sample_data/train_df_sampled_vapo.csv', index=False)\n",
    "test_df_sample[['text', 'response']].to_csv('./sample_data/test_df_sampled_vapo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6a14d",
   "metadata": {},
   "source": [
    "### Save datasets to a GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8747d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc8f2b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=BuQIh82reVjf7oKzKLYtzhxKZBwABO&access_type=offline&code_challenge=MMW3otBMJGOQJDXALJb0cQ4Zkko6k00rdlIGcWq4wgM&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/lexha/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"my-argolis-prj\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n",
      "\n",
      "\n",
      "Updates are available for some Google Cloud CLI components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1356e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from pathlib import Path\n",
    "\n",
    "def upload_jsonl_to_gcs(bucket_name: str, source_file_name: str, destination_blob_name: str):\n",
    "    \"\"\"\n",
    "    Uploads a local file (e.g., a .jsonl file) to a Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket (e.g., 'my-data-bucket').\n",
    "        source_file_name (str): The path to the local file to upload (e.g., 'data/input.jsonl').\n",
    "        destination_blob_name (str): The desired path/name of the file in the bucket \n",
    "                                     (e.g., 'inputs/2025/input.jsonl').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the source file exists\n",
    "    source_path = Path(source_file_name)\n",
    "    if not source_path.exists():\n",
    "        print(f\"Error: Source file not found at {source_file_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Initialize a GCS Client\n",
    "        # The client will automatically use Application Default Credentials (ADC)\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Get the bucket object\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Create a blob (file) object using the desired path in GCS\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "        # Upload the file\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "\n",
    "        print(\n",
    "            f\"File {source_file_name} uploaded successfully to \"\n",
    "            f\"gs://{bucket_name}/{destination_blob_name}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during upload: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec485fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BUCKET_NAME = \"your-unique-bucket-name\"  # <-- Replace with your bucket name\n",
    "LOCAL_JSONL_PATH = \"my_local_data.jsonl\"    # <-- Ensure this file exists\n",
    "GCS_DESTINATION_PATH = \"data_uploads/daily_run_20251002.jsonl\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a8112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define your parameters\n",
    "GCS_BUCKET_NAME = \"your-unique-bucket-name\"  # <-- Replace with your bucket name\n",
    "LOCAL_JSONL_PATH = \"my_local_data.jsonl\"    # <-- Ensure this file exists\n",
    "GCS_DESTINATION_PATH = \"data_uploads/daily_run_20251002.jsonl\" \n",
    "\n",
    "# 2. (Optional) Create a dummy JSONL file for testing\n",
    "# This ensures the script has a file to upload\n",
    "try:\n",
    "    with open(LOCAL_JSONL_PATH, 'w') as f:\n",
    "        f.write('{\"id\": 1, \"text\": \"This is line one.\"}\\n')\n",
    "        f.write('{\"id\": 2, \"text\": \"This is line two.\"}\\n')\n",
    "    print(f\"Created dummy file: {LOCAL_JSONL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create dummy file: {e}\")\n",
    "\n",
    "\n",
    "# 3. Call the upload function\n",
    "upload_jsonl_to_gcs(\n",
    "    bucket_name=GCS_BUCKET_NAME,\n",
    "    source_file_name=LOCAL_JSONL_PATH,\n",
    "    destination_blob_name=GCS_DESTINATION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774b765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
